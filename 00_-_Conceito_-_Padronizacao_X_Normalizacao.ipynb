{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35df150b",
   "metadata": {},
   "source": [
    "# <center>Normalização x Padronização</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853fde45",
   "metadata": {},
   "source": [
    "### O que é Normalização e quando aplicar ?\n",
    "\n",
    "- A **normalização** é uma técnica frequentemente aplicada à preparação dos dados em aprendizado de máquina. O **objetivo** da normalização é alterar os valores das colunas numéricas no conjunto de dados para uma **escala comum**, sem distorcer as diferenças nos intervalos de valores. Não precisamos aplicar normalização a todo conjunto de dados. É necessário apenas quando os recursos (variáveis) tiverem **intervalos diferentes**.\n",
    "- Por exemplo, considere o conjunto de dados contendo dois recursos, idade (x1) e receita (x2). Onde a faixa etária varia de 0 a 100 anos, enquanto a renda varia de 0 a 20.000 ou mais. A renda é cerca de 1.000 vezes maior do que a idade e com uma variação de valores muito maior. Então,  esses  dois  recursos  estão  em  intervalos  muito  **diferentes**.  Quando  fazemos  análises adicionais, como regressão linear multivariada, por exemplo, a renda atribuída influenciará muito mais o resultado devido ao seu valor maior. E isso causa problemas durante o treinamento do algoritmo.\n",
    "- A **normalização** também é chamada simplesmente de **Scaler Min-Max** e basicamente reduz o intervalo dos dados de forma que o intervalo seja fixo entre **0 e 1** (ou -1 a 1, se houver valores negativos). Funciona melhor para casos em que a padronização (que veremos no próximo item de aprendizagem) pode não funcionar tão bem. Se a distribuição não for gaussiana ou o desvio padrão for muito pequeno, o Scaler Min-Max funciona melhor.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Quando a Normalização é Importante?\n",
    "- A normalização é principalmente **necessária** no caso de algoritmos que usam medidas de distância como clustering, sistemas de recomendação que usam semelhança de cosseno, etc. Isto é feito de forma que uma variável que está em uma escala maior não afeta o resultado apenas porque está em uma escala maior.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Abaixo listamos alguns algoritmos de Machine Learning que requerem a normalização dos dados:\n",
    "\n",
    "- KNN com medida de distância euclidiana se quiser que todos os recursos contribuam igualmente no modelo.\n",
    "- Regressão Logística, SVM, Perceptrons, Redes Neurais.\n",
    "- K-Means\n",
    "- Análise  discriminante  linear,  análise  de  componentes  principais,  análise  de componentes principais do kernel.Classificadores baseados em modelo gráfico, como Fisher LDA ou Naive Bayes, bem como Árvores de Decisão e métodos baseados em árvore, como RandomForest, são invariantes ao dimensionamento de recursos, mas ainda assim pode ser uma boa ideia redimensionar os dados.\n",
    "\n",
    "A  normalização  eliminará  a  capacidade  de  interpretação  do  modelo  e,  portanto, dependerá, em última instância, da necessidade do negócio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b9ad0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### O Que é Padronização e quando aplicar?\n",
    "\n",
    "- A **Padronização** (ou **normalização do escore Z** ou em inglês **Standardization** ou ainda **Standard Scaler**) é o processo de redimensionamento dos recursos (variáveis) para que eles tenham as propriedades de uma **distribuição normal** com μ = 0 e σ = 1, onde μ é a média e σ é o desvio padrão da média.\n",
    "- É **amplamente utilizado** em SVMs, regressão logística e redes neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4f5c3",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Normalização x Padronização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d661b",
   "metadata": {},
   "source": [
    "- Conforme vimos  nas  aulas anteriores, a **Normalização** transforma os dados em um **intervalo**, digamos entre 0 e 1 ou 1 e 10, de forma que os números estejam na **mesma escala**. Por exemplo, podemos converter os dados de centímetros para metros para que tenhamos todos na mesma escala.\n",
    "\n",
    "<br>\n",
    "\n",
    "- A **Padronização** significa transformar os dados de tal forma que eles tenham **média zero e desvio padrão igual a 1**. Portanto, aqui temos os dados em escala de forma padronizada, de modo que a distribuição seja aproximadamente uma **distribuição normal**.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Quando usar ?\n",
    "\n",
    "- Ambas as técnicas têm suas desvantagens. Se você tiver valores **outliers** em seu conjunto de  dados, a **Normalização** dos  dados  certamente  aumentará  os dados \"normais\" para um intervalo muito pequeno. E, geralmente, a maioria dos conjuntos de dados tem outliers.\n",
    "- Ao usar a **Padronização**, seus novos dados não são limitados (ao contrário da Normalização).\n",
    "- Portanto, a **Normalização** é geralmente **evitada** quando o conjunto de dados tem **outliers** (desde que inclua o valor máximo). Nesses casos, preferimos a **Padronização**.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Algumas considerações importantes:\n",
    "- A **Normalização** torna o treinamento menos sensível à escala de recursos, para que possamos resolver melhor os coeficientes.\n",
    "- O uso de um método de **Normalização** melhorará a análise de múltiplos modelos.\n",
    "- A **Normalização** assegurará que um problema de convergência não tenha uma variância massiva, tornando a otimização viável.\n",
    "- A **Padronização** tende a tornar o processo de treinamento bem melhor, porque a condição numérica dos problemas de otimização é melhorada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d9438",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "A **padronização** e a **normalização** são técnicas de **escalonamento de dados** usadas para preparar dados para modelos de aprendizado de máquina, ajustando as escalas das variáveis para melhorar a eficácia dos algoritmos. Não faz sentido aplicar as duas técnicas.\n",
    "\n",
    "<br>\n",
    "\n",
    "Aqui estão as principais diferenças e orientações sobre quando usar cada técnica:\n",
    "\n",
    "#### Padronização\n",
    "- **Objetivo**: Redimensionar os recursos para que tenham média zero e desvio padrão de um.\n",
    "- **Quando usar**: Dados aproximadamente normalmente distribuídos.\n",
    "- **Algoritmos** que assumem uma distribuição normal dos dados, como SVM e regressão logística.\n",
    "- Dados com escalas diferentes, mas centralizados.\n",
    "- Útil para minimizar a influência de outliers.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Normalização\n",
    "- **Objetivo**: Ajustar os dados para que seus valores estejam dentro de um intervalo fixo, geralmente 0 a 1.\n",
    "- **Quando usar**: Dados com escalas muito diferentes.\n",
    "- **Algoritmos** que dependem de medidas de distância, como KNN e clustering.\n",
    "- Dados que não seguem uma distribuição normal ou quando a variância é pequena.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Resumo\n",
    "- **Padronização** é melhor para dados que já são centralizados e precisam de ajuste de escala sem vinculação a um intervalo específico.\n",
    "- **Normalização** é ideal para dados com grandes variações de escala e para algoritmos sensíveis à magnitude dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a369b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
