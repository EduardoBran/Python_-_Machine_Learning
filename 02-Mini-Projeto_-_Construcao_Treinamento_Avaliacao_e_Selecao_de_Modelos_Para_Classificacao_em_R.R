####  Big Data Real-Time Analytics com Python e Spark  ####

# Configurando o diret√≥rio de trabalho
setwd("~/Desktop/DataScience/CienciaDeDados/2.Big-Data-Real-Time-Analytics-com-Python-e-Spark/8.Machine_Learning_em_Linguagem_Python")
getwd()



## Importando Pacotes
library(readxl)         # carregar arquivos
library(dplyr)          # manipula dados
library(tidyr)          # manipula dados (funcao pivot_longer)
library(ROSE)           # balanceamento de dados
library(ggplot2)        # gera gr√°ficos
library(patchwork)      # unir gr√°ficos
library(corrplot)       # mapa de correla√ß√£o
library(caret)          # pacote preProcess para normaliza√ß√£o

library(randomForest)   # algoritmo de ML




############################  Modelagem Preditiva para Identifica√ß√£o de Risco de Doen√ßa Hep√°tica  ############################  

## Etapas:
  
# - Constru√ß√£o, Treinamento, Avalia√ß√£o e Sele√ß√£o de Modelos para Classifica√ß√£o

## Introdu√ß√£o:

# - Vamos trabalhar agora em nosso primeiro Mini-Projeto de Machine Learning, cujo objetivo √© fornecer um passo a passo completo do processo de constru√ß√£o,
#   treinamento, avalia√ß√£o e sele√ß√£o de modelos para classifica√ß√£o. Este projeto ser√° abordado de maneira integral, desde a defini√ß√£o do problema de neg√≥cio
#   at√© as previs√µes com o modelo treinado.

## Contexto:

# - O n√∫mero de pacientes com doen√ßa hep√°tica tem aumentado continuamente devido a fatores como consumo excessivo de √°lcool, inala√ß√£o de gases nocivos,
#   ingest√£o de alimentos contaminados, e uso de drogas e anabolizantes. Em resposta a essa crescente preocupa√ß√£o de sa√∫de p√∫blica, este mini-projeto visa 
#   construir um modelo de Machine Learning capaz de prever se um paciente ir√° ou n√£o desenvolver uma doen√ßa hep√°tica com base em v√°rias caracter√≠sticas 
#   cl√≠nicas e demogr√°ficas. Este modelo pode ser extremamente √∫til para m√©dicos, hospitais ou governos no planejamento de or√ßamentos de sa√∫de e na cria√ß√£o 
#   de pol√≠ticas de preven√ß√£o eficazes.

## Objetivo:

# - O objetivo √© prever uma classe (sim ou n√£o), usaremos aprendizado supervisionado para classifica√ß√£o, criando diferentes vers√µes do modelo com diferentes 
#   algoritmos e passaremos por todo o processo de Machine Learning de ponta a ponta. Usaremos como fonte de dados o dataset dispon√≠vel no link a seguir.

## Dados:

# Link -> https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)

# - Os dados para este projeto s√£o provenientes do "Indian Liver Patient Dataset", dispon√≠vel no link acima. Este conjunto de dados cont√©m registros de
#   pacientes hep√°ticos e n√£o hep√°ticos coletados na √çndia. A coluna "Dataset" atua como um r√≥tulo de classe, dividindo os indiv√≠duos em pacientes com doen√ßa
#   hep√°tica (1) ou sem a doen√ßa (2).



#### Carregando os Dados

df <- data.frame(read.csv2("dados/dataset.csv", sep = ","))

dim(df)
names(df)
head(df)



#### An√°lise Explorat√≥ria

# Tipo de dados
str(df)


## Aplicando Transforma√ß√µes Iniciais

# Converter todas as colunas para numeric, exceto 'Gender' (ficar igual ao c√≥digo em Python)
df <- df %>%
  mutate(across(-Gender, as.numeric))


# Atualizar valores na coluna 'Dataset' (valor '2' = '0') e renomear para 'Target'
df <- df %>%
mutate(Dataset = if_else(Dataset == 2, 0, Dataset)) %>%
  rename(Target = Dataset)


# Conveter para Factor as Vari√°veis Gender e Target(vari√°vel alvo)
df <- df %>% 
  mutate(Gender = as.factor(Gender),
         Target = as.factor(Target))



## Realizando An√°lise Inicial (Sum√°rio Estat√≠stico, Veririca√ß√£o de Valores NA, '' e especiais)

analise_inicial <- function(dataframe_recebido) {
  # Sum√°rio
  cat("\n\n####  DIMENS√ïES  ####\n\n")
  print(dim(dataframe_recebido))
  cat("\n\n\n####  INFO  ####\n\n")
  print(str(dataframe_recebido))
  cat("\n\n\n####  SUM√ÅRIO  ####\n\n")
  print(summary(dataframe_recebido))
  cat("\n\n\n####  VERIFICANDO QTD DE LINHAS DUPLICADAS  ####\n\n")
  print(sum(duplicated(dataframe_recebido)))
  cat("\n\n\n####  VERIFICANDO VALORES NA  ####\n\n")
  valores_na <- colSums(is.na(dataframe_recebido))
  if(any(valores_na > 0)) {
    cat("\n-> Colunas com valores NA:\n\n")
    print(valores_na[valores_na > 0])
  } else {
    cat("\n-> N√£o foram encontrados valores NA.\n")
  }
  cat("\n\n\n####  VERIFICANDO VALORES VAZIOS ''  ####\n\n")
  valores_vazios <- sapply(dataframe_recebido, function(x) sum(x == "", na.rm = TRUE)) # Adicionando na.rm = TRUE
  if(any(valores_vazios > 0, na.rm = TRUE)) { # Tratamento de NA na condi√ß√£o
    cat("\n-> Colunas com valores vazios \"\":\n\n")
    print(valores_vazios[valores_vazios > 0])
  } else {
    cat("\n-> N√£o foram encontrados valores vazios \"\".\n")
  }
  cat("\n\n\n####  VERIFICANDO VALORES COM CARACTERES ESPECIAIS  ####\n\n")
  caracteres_especiais <- sapply(dataframe_recebido, function(x) {
    sum(sapply(x, function(y) {
      if(is.character(y) && length(y) == 1) {
        any(charToRaw(y) > 0x7E | charToRaw(y) < 0x20)
      } else {
        FALSE
      }
    }))
  })
  if(any(caracteres_especiais > 0)) {
    cat("\n-> Colunas com caracteres especiais:\n\n")
    print(caracteres_especiais[caracteres_especiais > 0])
  } else {
    cat("\n-> N√£o foram encontrados caracteres especiais.\n")
  }
}

analise_inicial(df)



## Dividindo dataset em vari√°veis num√©ricas e categ√≥ricas (para cria√ß√£o de gr√°ficos)
df_num <- df %>% 
  select(where(is.numeric))
df_cat <- df %>% 
  select(where(is.factor))



### Explorando Vari√°veis Num√©ricas

# Sum√°rio Estat√≠stico
summary(df_num)


## Visualizando Atrav√©s de Gr√°ficos

# Converter o dataframe para um formato longo para facilitar a plotagem com ggplot2
df_long <- pivot_longer(df_num, cols = everything())

# Criar histogramas usando ggplot2
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 10, fill = "blue", color = "black") +
  facet_wrap(~name, scales = "free") +
  labs(x = "Value", y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Melhorar a legibilidade dos r√≥tulos do eixo x


# Interpretando sum√°rio e gr√°ficos:

# - Parece que h√° outlier nas vari√°veis "Alamine_Aminotransferase" e "Aspartate_Aminotransferase", pois o valor m√°ximo √© muito mais alto que o valor m√©dio.




### Explorando Vari√°vel Categ√≥rica

str(df_cat)
summary(df_cat)
plot(df$Gender)
plot(df$Target)


## Aplicando Label Encoding na Vari√°vel 'Gender' e 'Target' (no dataframe original df)

str(df)

# Cria uma Nova Vari√°vel 'Gender_Num' onde Male = 0 e Female = 1
# df$Gender_Num <- ifelse(df$Gender == "Male", 0, 1)

# Altera a vari√°vel original
df$Gender <- ifelse(df$Gender == "Male", 0, 1)

# Converte fatores para caracteres e depois para num√©ricos
df$Target <- as.numeric(as.character(df$Target))

str(df)



## Verificando Correla√ß√£o

#df_num <- df %>% 
#  select(-Gender)
cor(df, use = "complete.obs")

# Criar um mapa de calor da matriz de correla√ß√£o
corrplot(cor(df, use = "complete.obs"),
         method = "color",
         type = "upper",
         addCoef.col = 'springgreen2',
         tl.col = "black",
         tl.srt = 45)                                     # Esconde a diagonal principal


## Interpretando o resultado da Correla√ß√£o

# - Vamos citar exemplo:
#   Podemos constatar no dados e gr√°fico que a vari√°vel Total_Bilirubin tem uma alta correla√ß√£o positiva com a vari√°vel Direct_Bilirubin (0.87).
# - Isso √© um problema pois a mesma informa√ß√£o est√° sendo replicada duas vezes e por conta disso pode deixar o modelo tendencioso.
# - O fato de duas vari√°veis estarem altamente relacionadas (quando tem o valor abaixo ou acima de 0.70) √© chamado de Multicolinearidade. 
# - Em algum momento deveremos tomar uma decis√£o: deixar as duas vari√°veis, remover uma vari√°vel ou remover as duas.

## Aten√ß√£o

# - Nosso dados ainda n√£o foram limpos/tratados (valores ausentes, replicados ou outliers). √â recomendado aplicar algum tipo de tratamento relacionado a Multicolinearidade somente quando os dados estiverem tratados.
# - Estamos na etapa de An√°lise Explorat√≥ria onde estamos entendendo a natureza dos nossos dados.


## Verificando Rela√ß√£o entre Atributs

## Verificando Atrav√©s de Gr√°fico a Rela√ß√£o entre as Vari√°veis 'Direct_Bilirubin' e 'Total_Bilirubin' por 'Target'
ggplot(df, aes(x = Total_Bilirubin, y = Direct_Bilirubin, color = as.factor(Target))) +
  geom_point(alpha = 0.6, size = 3) +  # Pontos semitransparentes e de tamanho moderado
  scale_color_manual(values = c("blue", "red")) +  # Cores para diferentes Targets
  labs(color = "Target") +  # Legenda
  theme_minimal(base_size = 14) +  # Usando um tema minimalista para o background
  theme(
    plot.background = element_rect(fill = "grey90"),  # Cor de fundo do plot
    panel.background = element_rect(fill = "grey90", colour = "grey20", size = 0.5, linetype = "solid"),
    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = "grey60"),  # Linhas principais da grade
    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = "grey80")  # Linhas secund√°rias da grade
  ) +
  labs(title = "Rela√ß√£o entre Total Bilirubin e Direct Bilirubin", x = "Total Bilirubin", y = "Direct Bilirubin")


## Verificando Atrav√©s de Gr√°fico a Rela√ß√£o entre as Vari√°veis 'Direct_Bilirubin' e 'Total_Bilirubin' por 'Gender'
ggplot(df, aes(x = Total_Bilirubin, y = Direct_Bilirubin, color = as.factor(Gender))) +
  geom_point(alpha = 0.6, size = 3) +  # Pontos semitransparentes e de tamanho moderado
  scale_color_manual(values = c("blue", "red")) +  # Cores para diferentes Targets
  labs(color = "Gender") +  # Legenda
  theme_minimal(base_size = 14) +  # Usando um tema minimalista para o background
  theme(
    plot.background = element_rect(fill = "grey90"),  # Cor de fundo do plot
    panel.background = element_rect(fill = "grey90", colour = "grey20", size = 0.5, linetype = "solid"),
    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = "grey60"),  # Linhas principais da grade
    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = "grey80")  # Linhas secund√°rias da grade
  ) +
  labs(title = "Rela√ß√£o entre Total Bilirubin e Direct Bilirubin", x = "Total Bilirubin", y = "Direct Bilirubin")


## Verificando Atrav√©s de Gr√°fico a Rela√ß√£o entre as Vari√°veis 'Albumin' e 'Total_Bilirubin' por 'Target'
ggplot(df, aes(x = Total_Bilirubin, y = Albumin, color = as.factor(Target))) +
  geom_point(alpha = 0.6, size = 3) +  # Pontos semitransparentes e de tamanho moderado
  scale_color_manual(values = c("blue", "red")) +  # Cores para diferentes Targets
  labs(color = "Target") +  # Legenda
  theme_minimal(base_size = 14) +  # Usando um tema minimalista para o background
  theme(
    plot.background = element_rect(fill = "grey90"),  # Cor de fundo do plot
    panel.background = element_rect(fill = "grey90", colour = "grey20", size = 0.5, linetype = "solid"),
    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = "grey60"),  # Linhas principais da grade
    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = "grey80")  # Linhas secund√°rias da grade
  ) +
  labs(title = "Rela√ß√£o entre Total Bilirubin e Albumin", x = "Total Bilirubin", y = "Albumin")


## Verificando Atrav√©s de Gr√°fico a Rela√ß√£o entre as Vari√°veis 'Albumin' e 'Total_Bilirubin' por 'Gender'
ggplot(df, aes(x = Total_Bilirubin, y = Albumin, color = as.factor(Gender))) +
  geom_point(alpha = 0.6, size = 3) +  # Pontos semitransparentes e de tamanho moderado
  scale_color_manual(values = c("blue", "red")) +  # Cores para diferentes Targets
  labs(color = "Gender") +  # Legenda
  theme_minimal(base_size = 14) +  # Usando um tema minimalista para o background
  theme(
    plot.background = element_rect(fill = "grey90"),  # Cor de fundo do plot
    panel.background = element_rect(fill = "grey90", colour = "grey20", size = 0.5, linetype = "solid"),
    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = "grey60"),  # Linhas principais da grade
    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = "grey80")  # Linhas secund√°rias da grade
  ) +
  labs(title = "Rela√ß√£o entre Total Bilirubin e Albumin", x = "Total Bilirubin", y = "Albumin")



#### Conclus√µes da An√°lise Explorat√≥ria

# - A an√°lise explorat√≥ria ajudou a entender a natureza dos dados, preparando o caminho para limpeza de dados e an√°lises mais profundas.
# - Identificou-se a necessidade de tratar valores ausentes e poss√≠veis outliers.
# - A an√°lise de correla√ß√£o destacou a presen√ßa de multicolinearidade, que pode afetar a performance de modelos de aprendizado de m√°quina.





#### Verfificando e Tratando Valores Ausentes, Replicados e Outliers

analise_inicial(df)


## Verificando Valores Ausentes (o tratamento √© aconselhado ap√≥s tratamento dos valores outliers)

# Exibindo as linhas com valores ausentes
df %>% filter(is.na(Albumin_and_Globulin_Ratio))



## Tratando Valores Duplicados

# Exibindo as linhas com valores duplicados
df %>% filter(duplicated(.))

# Removendo linhas duplicadas (remove uma das duplicatas)
df <- df %>% 
  distinct()



## Tratando Valores Outliers

# - Ir√° ser apresentado dois cen√°rios e tomaremos decis√µes diferentes para cada um deles.


## Cen√°rio 1 (Vari√°vel 'Alamine_Aminotransferase')

# Sum√°rio
summary(df$Alamine_Aminotransferase)

# - Atrav√©s do sum√°rio, podemos observar que a vari√°vel possui uma m√©dia de 80.14 e um valor m√°x de 2000. Isso √© um sinal de que podemos ter um outlier.
# - Podemos checar esta informa√ß√£o atrav√©s da cria√ß√£o de um Gr√°fico BoxPlot.


# Gr√°fico BoxPLot
ggplot(df, aes(y = Alamine_Aminotransferase)) +
  geom_boxplot(fill = "blue", colour = "black") +
  labs(title = "Boxplot de Alamine Aminotransferase", y = "Alamine Aminotransferase") +
  theme_minimal()  # Aplicando um tema minimalista

# Interpretando o Gr√°fico

# - Podemos verificar que al√©m do valor de 2000 temos outros diversos valores acima da m√©dia (que est√° pr√≥xima de zero).
# - Ser√° que os valores extremos s√£o mesmo outliers para esta vari√°vel?
  
# Podemos responder isso verificando contagem de frequ√™ncia por valor abaixo (filtrando os 5 maiores valores):


# Exibindo os cinco maiores valores √∫nicos e suas frequ√™ncias:
table(df$Alamine_Aminotransferase)[as.character(sort(unique(df$Alamine_Aminotransferase), decreasing = TRUE)[1:5])]

# Exibindo a quantidade de valores acima da m√©dia:
sum(df$Alamine_Aminotransferase > mean(df$Alamine_Aminotransferase))  # Contagem de valores acima da m√©dia
length(df$Alamine_Aminotransferase)                                   # Contagem total de valores da vari√°vel


# Conclus√£o

# - Ap√≥s a an√°lise detalhada da vari√°vel 'Alamine_Aminotransferase', identificamos que o valor m√°ximo de 2000 √© consideravelmente mais alto que os outros
#   valores pr√≥ximos, que tamb√©m s√£o altos mas menos frequentes.
# - Esses valores extremos podem ser considerados outliers devido ao seu afastamento significativo da m√©dia e mediana, al√©m de serem raros no dataset,
#   como mostrado pela an√°lise de frequ√™ncia.

# - Dado esse contexto, √© sugerido a avalia√ß√£o de tratamento desses outliers dentro do cen√°rio de aplica√ß√£o dos dados. Se estes valores s√£o resultantes de
#   erros de medi√ß√£o ou casos muito at√≠picos que podem distorcer an√°lises estat√≠sticas, a remo√ß√£o ou substitui√ß√£o por um limite superior calculado pelo
#   m√©todo do IQR √© recomendada.
# - Contudo, se esses altos valores representam casos v√°lidos dentro da pesquisa ou aplica√ß√£o pr√°tica dos dados, poderiam ser mantidos, mas com uma an√°lise
#   adicional para confirmar sua validade.

# - Portanto neste caso espec√≠fico, ap√≥s verificar a validade dos dados, optou-se por n√£o realizar o tratamento de outliers para esta vari√°vel, pois eles
#   representam casos aut√™nticos dentro do contexto estudado.



## Cen√°rio 2 (Vari√°vel 'Aspartate_Aminotransferase')

# Sum√°rio
summary(df$Aspartate_Aminotransferase)

# - Atrav√©s do Sum√°rio podemos observar que a vari√°vel possui uma m√©dia de 109.89 e um valor m√°x de 4929. Isso √© um sinal de que podemos ter um ou mais
#   valores outlier.
# - Vamos novamente verificar por um Gr√°fico BoxPlot.

# Gr√°fico BoxPLot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
  geom_boxplot(fill = "blue", colour = "black") +
  labs(title = "Boxplot de Aspartate Aminotransferase", y = "Aspartate Aminotransferase") +
  theme_minimal()  # Aplicando um tema minimalista

# Interpretando o gr√°fico

# - Podemos verificar que novamente temos valores outliers, mas com um comportamente diferente. Parece que temos menos dados com valores extremos.
# - Aqui n√≥s temos apenas dois valores outliers acima de 2000 enquanto todos os outros abaixo deste valor. 
# - E neste caso, todos esses valores extremos s√£o mesmo outliers para esta vari√°vel?
  
# Podemos responder isso verificando novamente os maiores valores √∫nicos e suas frequ√™ncias:


# Exibindo os cinco maiores valores √∫nicos e suas frequ√™ncias:
table(df$Aspartate_Aminotransferase)[as.character(sort(unique(df$Aspartate_Aminotransferase), decreasing = TRUE)[1:5])]

# Exibindo a quantidade de valores acima da m√©dia:
sum(df$Aspartate_Aminotransferase > mean(df$Aspartate_Aminotransferase))  # Contagem de valores acima da m√©dia
length(df$Aspartate_Aminotransferase)                                     # Contagem total de valores da vari√°vel

# Exibindo a quantidade de valores acima de 2000:
sum(df$Aspartate_Aminotransferase > 2000)                                 # Contagem de valores acima de 2000
length(df$Aspartate_Aminotransferase)                                     # Contagem total de valores da vari√°vel


# Conclus√£o

# - Vamos aplicar um tratamento para limpeza de outlier nesta vari√°vel.
# - Iremos manter no dataset todos os registros abaixo do valor 2500 para esta vari√°vel.


# Tratando Valores Outliers da Vari√°vel 'Alamine_Aminotransferase'

dim(df)
summary(df)

# Aplica tratamento mantendo somente os registros onde o valor for menor ou igual a 3000 e verifica shape
df <- df %>% filter(Aspartate_Aminotransferase <= 3000)
dim(df)

# BoxPlot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
  geom_boxplot(fill = "blue", colour = "black") +
  labs(title = "Boxplot de Aspartate Aminotransferase Ap√≥s Primeiro Filtro", y = "Aspartate Aminotransferase")

# Aplica novo tratamento mantendo somente os registros onde o valor for menor ou igual a 2500 e verifica shape 
df <- df %>% filter(Aspartate_Aminotransferase <= 2500)

# BoxPlot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
  geom_boxplot(fill = "blue", colour = "black") +
  labs(title = "Boxplot de Aspartate Aminotransferase Ap√≥s Segundo Filtro", y = "Aspartate Aminotransferase")


dim(df)
summary(df)


## Tratando Valores Ausentes
dim(df)

# Removendo todas linhas com valores ausentes
df <- df %>% drop_na()
dim(df)



#### RESUMO

# - Antes de avan√ßarmos para a etapa final de pr√©-processamento de dados, crucial para a constru√ß√£o de modelos de machine learning, vamos recapitular
#   os passos j√° conclu√≠dos no projeto.:
  
#  -> Primeiro foi definido o problema de neg√≥cio para saber o objetivo e o que temos que resolver.
#  -> Depois n√≥s extra√≠mos os dados e nesta etapa pode ser que tenhamos o suporte de um Engenheiro de Dados. No caso deste projeto foi feito a leitura dos
#     dados atrav√©s de um arquivo csv.
#  -> Na sequ√™ncia foi feita a An√°lise Explorat√≥ria onde n√≥s verificamos padr√µes, detectamos problemas, identifica coisas que precisamos fazer.
#  -> Ap√≥s isso √© aplicado a Limpeza de Dados de acordo com as t√©cnicas necess√°rias, estrat√©gias e decis√µes.
#  -> Sempre lembrar de documentar tudo o que foi feito em cada atividade.





#### Pr√©-Processamento de Dados Para Constru√ß√£o de Modelos de Machine Learning¬∂

# - Como vimos anteriormente ao aplicarmos o mapa de correla√ß√£o as vari√°veis 'Direct_Bilirubin' e 'Total_Bilirubin' possuem uma alta correla√ß√£o.
# - Com isso foi tomada a decis√£o de remover umas das vari√°veis.

# Removendo Vari√°vel 'Direct_Bilirubin'
df <- df %>% 
  select(-Direct_Bilirubin)


##  Dividindo os dados em treino e teste

set.seed(100)
indices <- createDataPartition(df$Target, p = 0.75, list = FALSE)
dados_treino <- df[indices, ]
dados_teste <- df[-indices, ]
rm(indices)



## Balanceamento de Classes

# Contagem
table(dados_treino$Target)

# Por que realizar o Balanceamento de Classes ?

# - Como foi observado no table() acima podemos constatar que os dados est√£o desbalanceados, isso signifca que tem muito mais pacientes de uma classe do
#   que da outra.
# - E o que acontece quando n√£o realizarmos o balanceamento? O modelo de ML aprender√° muito mais o padr√£o da Classe 1 do que da Classe 0.
# - Caso n√£o aplicamos t√©cnica de Balanceamento, o modelo tende a ficar tendencioso. Por isso precisamos fazer o Balanceamento de Classes.

# Estrat√©gias para o Balanceamento

# Temos duas estrat√©gias:

# - Reduzir os registros da classe majorit√°ria e assim diminuir consideravelmente o n√∫mero de registros no nosso dataset.
# - Aplicar a t√©cnica de Oversampling onde ir√° ser aumentado o n√∫mero de registros das classe minorit√°ria. E como isso √© feito? Sendo criado dados
#   sint√©ticos com base nos dados existentes (Para isso, podemos utilizar o pacote ROSE em R, que oferece fun√ß√µes para gerar dados sint√©ticos).


# Balanceamento da Vari√°vel Alvo (Aplicando a t√©cnica Oversampling para balancear a vari√°vel alvo)
table(dados_treino$Target)
dados_balanceados <- ovun.sample(Target ~ ., data = dados_treino, method = "over", N = 2*max(table(dados_treino$Target)))$data
table(dados_balanceados$Target)


# Por que a t√©cnica de oversamping dever se aplicada somente nos dados de treino?

# - A t√©cnica de oversampling deve ser aplicada somente nos dados de treino para evitar o vazamento de dados (data leakage) e garantir uma avalia√ß√£o justa
#   e realista do modelo durante o teste.
# - Se o balanceamento fosse aplicado ao conjunto de dados completo, incluindo os dados de teste, o modelo poderia acabar sendo avaliado com dados
#   sint√©ticos, n√£o representativos da realidade, influenciando os resultados dos testes e comprometendo a capacidade de generaliza√ß√£o do modelo para novos
#   dados n√£o vistos.
# - Portanto, mantendo o conjunto de teste original, sem dados sint√©ticos, asseguramos que a performance do modelo reflete melhor sua efic√°cia em cen√°rios
#   reais.


# Tamanho
dim(dados_treino)
dim(dados_balanceados)

# O dataset de treino agora passou de 423 linhas para 608 linhas.

# Ajusta o nome do dataset de treino
dados_treino <- dados_balanceados
rm(dados_balanceados)

# Contagem
table(dados_treino$Target)



### Padroniza√ß√£o x Normaliza√ß√£o

# As t√©cnicas de padroniza√ß√£o e normaliza√ß√£o s√£o usadas no pr√©-processamento de dados em aprendizado de m√°quina para preparar vari√°veis num√©ricas,
# ajustando suas escalas. Aqui est√° quando e por que usar cada uma:
  
## Padroniza√ß√£o
# Transforma os dados de modo que eles tenham m√©dia zero e desvio padr√£o igual a um.

# - Quando usar    : Aplic√°vel quando os dados j√° est√£o centralizados em torno de uma m√©dia e precisam de ajuste na escala. √â √∫til em modelos como SVM e
#                    Regress√£o Log√≠stica, que s√£o sens√≠veis a varia√ß√µes na escala das vari√°veis de entrada.
# - Exemplo pr√°tico: Se medimos altura em cent√≠metros (150-190 cm) e peso em quilogramas (50-100 kg), a padroniza√ß√£o permite comparar essas medidas numa 
#                    escala comum, evitando distor√ß√µes devido a diferentes intervalos de valores.
# - Motivo para
#   este projeto   : Optamos pela padroniza√ß√£o porque as vari√°veis t√™m escalas muito diferentes e h√° a presen√ßa de outliers significativos. A padroniza√ß√£o
#                    mant√©m as propriedades estat√≠sticas dos dados, minimizando o impacto dos outliers, ao contr√°rio da normaliza√ß√£o que pode distorcer os
#                    dados ao comprimir a maioria dos valores em um intervalo estreito.

## Normaliza√ß√£o
# Ajusta os dados para que seus valores caibam em um intervalo predefinido, geralmente de 0 a 1.

# - Quando usar    : Ideal para dados com varia√ß√µes extremas nas escalas e onde os algoritmos s√£o sens√≠veis √† magnitude absoluta dos dados, como
#                    K-Nearest Neighbors (KNN) e t√©cnicas de clustering.
# - Exemplo pr√°tico: Se um dataset cont√©m pre√ßos de produtos variando de R 1ùëéùëÖ1000 e quantidades vendidas de 1 a 20 unidades, a normaliza√ß√£o faria com
#                    que ambos os atributos tivessem a mesma contribui√ß√£o no modelo, independentemente da escala original.
# - Motivo para n√£o
#   esta no projeto: N√£o foi escolhida devido √† presen√ßa de outliers, que poderiam ser enfatizados indevidamente, e porque a normaliza√ß√£o poderia limitar
#                    a efic√°cia de modelos que assumem uma distribui√ß√£o normal dos dados.

## Importante:

# - N√£o √© necess√°rio aplicar padroniza√ß√£o/normaliza√ß√£o na vari√°vel alvo.
# - N√≥s n√£o aplicamos as duas t√©cnicas, ou usamos uma ou outra.
# - A normaliza√ß√£o pode n√£o ser a melhor escolha se houver outliers significativos no conjunto de dados, pois isso poderia comprimir a maioria dos dados
#   em um intervalo muito estreito. Nesses casos, a padroniza√ß√£o √© recomendada.


# Padronizado Dados de Treino
summary(dados_treino)

# Calculando a m√©dia e o desvio padr√£o dos dados de treino 
treino_mean <- sapply(dados_treino[, -which(names(dados_treino) == "Target")], mean, na.rm = TRUE)
treino_std <- sapply(dados_treino[, -which(names(dados_treino) == "Target")], sd, na.rm = TRUE)

# Exibindo a m√©dia e o desvio padr√£o
print(treino_mean)
print(treino_std)

# Padronizando todas as vari√°veis, exceto 'Target'
dados_treino[, names(treino_mean)] <- sweep(dados_treino[, names(treino_mean)], 2, treino_mean, "-")
dados_treino[, names(treino_std)] <- sweep(dados_treino[, names(treino_std)], 2, treino_std, "/")

summary(dados_treino)


# Padronizado Dados de Teste
summary(dados_teste)

# Padronizando os dados de teste usando a m√©dia e desvio padr√£o dos dados de treino
dados_teste[, names(treino_mean)] <- sweep(dados_teste[, names(treino_mean)], 2, treino_mean, "-")
dados_teste[, names(treino_std)] <- sweep(dados_teste[, names(treino_std)], 2, treino_std, "/")

summary(dados_teste)

rm(treino_mean, treino_std)









#### Separando Dados de Treino e Teste (Python X R)

## No R:
# - √â comum especificar a vari√°vel alvo diretamente nos modelos ou fun√ß√µes de treinamento. Por exemplo, ao usar o pacote caret ou fun√ß√µes nativas como lm()
#   para regress√£o linear, voc√™ normalmente formula o modelo dentro da fun√ß√£o, como em lm(y ~ ., data = dados_treino), onde y √© a vari√°vel alvo e
#   indica o uso de todas as outras vari√°veis no dataframe como preditores.
# - Isso significa que n√£o h√° necessidade estrita de separar fisicamente a vari√°vel alvo das demais vari√°veis antes do treinamento do modelo.

## No Python:
# - Ao usar bibliotecas como scikit-learn, voc√™ geralmente precisa passar explicitamente os arrays ou matrizes de caracter√≠sticas e a vari√°vel alvo
#   separadamente para as fun√ß√µes de treinamento. Por exemplo, ao treinar um regressor log√≠stico, voc√™ usaria algo como LogisticRegression().fit(X_treino,
#   y_treino). Aqui, X_treino e y_treino s√£o passados como argumentos separados, o que requer que voc√™ prepare esses objetos com anteced√™ncia.
# - Em Python, mesmo que voc√™ esteja usando uma biblioteca que permite formula√ß√µes mais semelhantes ao R (como statsmodels), a pr√°tica comum e a maioria
#   das APIs de machine learning ainda segue o padr√£o de passar X e y separadamente.

# Por que isso √© feito dessa forma em Python?

# - A separa√ß√£o expl√≠cita de X e y fornece clareza e evita erros em um ecossistema que √© menos integrado do que o R para an√°lises estat√≠sticas.
#   As bibliotecas de Python, como scikit-learn, s√£o projetadas para serem agn√≥sticas quanto ao tipo de dados, permitindo o trabalho com arrays numpy,
#   dataframes pandas, e outros formatos de dados, de uma maneira altamente modular e flex√≠vel. Al√©m disso, essa separa√ß√£o ajuda na implementa√ß√£o de uma
#   variedade de pr√©-processamentos e transforma√ß√µes de maneira mais controlada e sem risco de alterar inadvertidamente a vari√°vel alvo.

# Essas diferen√ßas refletem filosofias de design distintas e t√™m implica√ß√µes pr√°ticas na maneira como voc√™ prepara e manipula dados para an√°lises e
# modelagem em cada linguagem.

