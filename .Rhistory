# Extraindo os melhores parâmetros
best_lambda <- grid_search$bestTune$lambda
best_lambda
# Treinando o modelo final com os melhores parâmetros encontrados
modelo_v1 <- glmnet(
x = as.matrix(dados_treino[, -which(names(dados_treino) == "Target")]),
y = as.numeric(dados_treino$Target),
alpha = 0,  # L2 penalidade como em LogisticRegression com 'l2'
lambda = best_lambda,
family = "binomial",
standardize = TRUE  # Equivalente ao StandardScaler
)
modelo_v1
rm(train_control, lambda_grid, tuned_params, grid_search)
## Previsões
# Preparando dados de teste para previsão
X_teste <- as.matrix(dados_teste[, -which(names(dados_teste) == "Target")])
# Previsões de Classe
y_pred_v1 <- predict(modelo_v1, newx = X_teste, s = "lambda.min", type = "class")
y_pred_v1 <- as.factor(ifelse(y_pred_v1 == "1", "Class0", "Class1"))
head(y_pred_v1)
# Previsões de Probabilidade
y_pred_proba_v1 <- predict(modelo_v1, newx = X_teste, s = "lambda.min", type = "response")
head(y_pred_proba_v1)
## Avaliação do Modelo
# Matriz de Confusão
conf_matrix <- confusionMatrix(y_pred_v1, dados_teste$Target)
conf_matrix
# Calcula e exibe a métrica AUC-ROC
roc_obj <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1))
roc_auc_v1 <- auc(roc_obj)
roc_auc_v1
# Calcula a curva ROC
roc_curve <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1))
roc_curve
plot(roc_curve, main="ROC Curve", col="#1c61b6") # Opcional: Gráfico da curva ROC
# Calcula e exibe a acurácia
acuracia_v1 <- sum(y_pred_v1 == dados_teste$Target) / length(y_pred_v1)
acuracia_v1
# Exibindo os resultados
cat("Confusion Matrix:\n")
print(conf_matrix$table)
cat(sprintf("\nAUC-ROC: %f\n", roc_auc_v1))
cat(sprintf("Accuracy: %f\n", acuracia_v1))
## Salvando as métricas do modelo_v1 em um Dicionário
dict_modelo_v1 <- data.frame(
Nome = "modelo_v1",
Algoritmo = "Regressão Logística",
ROC_AUC_Score = as.numeric(roc_auc_v1),  # Converte AUC para numérico
AUC_Score = as.numeric(auc(roc_curve)),  # AUC calculada da curva ROC, também convertida
Acuracia = acuracia_v1
)
## Adiciona o Dicionário com resultado das métricas do modelo_v1 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v1)
df_modelos
rm(X_teste, y_pred_v1, y_pred_proba_v1, conf_matrix, roc_obj, acuracia_v1, roc_auc_v1, roc_curve, dict_modelo_v1)
## Versão 2
# - Aplica a Técnica de Feature Selection no modelo_v1 criado na Versão 1
# - Re-cria o modelo utilizando as 5 variáveis mais importantes
# Extraindo coeficientes do modelo para o melhor lambda
coeficientes <- as.matrix(coef(modelo_v1, s = best_lambda))
rownames(coeficientes) <- c("(Intercept)", names(dados_treino)[-which(names(dados_treino) == "Target")])
# Calculando a importância como o valor absoluto dos coeficientes
importancias <- abs(coeficientes[-1, , drop = FALSE])  # Exclui o intercepto
df_importancias <- data.frame(
Feature = rownames(importancias),
Importance = importancias[, 1]
)
df_importancias <- df_importancias[order(-df_importancias$Importance), ]
# Visualizando por Números
print(df_importancias)
# Visualiando por Gráfico
ggplot(df_importancias, aes(x = Importance, y = reorder(Feature, Importance))) +
geom_bar(stat = "identity", fill = "skyblue", orientation = "y") +
labs(title = "Importância das Variáveis", x = "Importância", y = "Variável") +
theme_minimal() +
theme(axis.text.y = element_text(angle = 0, hjust = 1))  # Melhorando a legibilidade dos rótulos
## Selecionando as 5 variáveis mais importantes
# Critério: Foi detectado uma disparidade entre as 5 primeiras e as outras 4 variáveis. Vamos escolher as 5 primerias.
vars_importantes <- head(df_importancias$Feature, 5)
## Recriando o modelo usando apenas as variáveis mais importantes
dados_treino_importantes <- dados_treino[, c(vars_importantes, "Target")]
# Recriando modelo_v2 com variáveis selecionadas
modelo_v2 <- glmnet(
x = as.matrix(dados_treino_importantes[, -which(names(dados_treino_importantes) == "Target")]),
y = as.numeric(dados_treino_importantes$Target),
alpha = 0,  # L2 penalidade como em LogisticRegression com 'l2'
lambda = best_lambda,
family = "binomial",
standardize = TRUE
)
modelo_v2
## Preparando dados de teste para previsão com as variáveis mais importantes
dados_teste_importantes <- dados_teste[, c(vars_importantes, "Target")]
X_teste_importantes <- as.matrix(dados_teste_importantes[, -which(names(dados_teste_importantes) == "Target")])
## Previsões de Classe
y_pred_v2 <- predict(modelo_v2, newx = X_teste_importantes, s = "lambda.min", type = "class")
y_pred_v2 <- as.factor(ifelse(y_pred_v2 == "1", "Class0", "Class1"))
# Previsões de Probabilidade
y_pred_proba_v2 <- predict(modelo_v2, newx = X_teste_importantes, s = "lambda.min", type = "response")
## Avaliação do Modelo
conf_matrix_v2 <- confusionMatrix(y_pred_v2, dados_teste_importantes$Target)
roc_obj_v2 <- roc(response = dados_teste_importantes$Target, predictor = as.numeric(y_pred_proba_v2))
roc_auc_v2 <- auc(roc_obj_v2)
acuracia_v2 <- sum(y_pred_v2 == dados_teste_importantes$Target) / length(y_pred_v2)
# Salvando as métricas do modelo_v2 em um Dicionário
dict_modelo_v2 <- data.frame(
Nome = "modelo_v2",
Algoritmo = "Regressão Logística com Seleção de Variáveis",
ROC_AUC_Score = as.numeric(roc_auc_v2),
AUC_Score = as.numeric(auc(roc_obj_v2)),
Acuracia = acuracia_v2
)
# Adiciona o Dicionário com resultado das métricas do modelo_v2 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v2)
df_modelos
rm(modelo_v1, modelo_v2, dados_teste_importantes, X_teste_importantes, importancias, y_pred_v2, y_pred_proba_v2, conf_matrix_v2, roc_obj_v2,
roc_auc_v2, acuracia_v2, dict_modelo_v2, best_lambda, coeficientes, df_importancias, vars_importantes, dados_treino_importantes)
###  Modelo 2 com Random Forest
# - Nosso desafio agora é tentar obter um modelo melhor que a versão 1. Vamos tentar o algoritmo Random Forest.
## Versão 1
# - Criação e treinamento do modelo com Random Forest com a utilzação de hiperparâmetros
set.seed(123)
# Definir os hiperparâmetros
tuned_params_v2 <- list(
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Lista para armazenar os resultados
resultados <- list()
# Loop sobre os hiperparâmetros
for (n in tuned_params_v2$n_estimators) {
for (split in tuned_params_v2$min_samples_split) {
for (leaf in tuned_params_v2$min_samples_leaf) {
# Treinar o modelo com os hiperparâmetros atuais
modelo <- randomForest(
formula = Target ~ .,  # Definir sua fórmula aqui
data = dados_treino,  # Seus dados de treinamento
ntree = n,  # Número de árvores na floresta
mtry = split,  # Número de variáveis a serem consideradas em cada divisão
min.node.size = leaf  # Tamanho mínimo do nó
)
# Salvar os resultados
resultados[[paste("ntree", n, "mtry", split, "min.node.size", leaf)]] <- modelo
}
}
}
# Extraindo melhor configuração de hiperparâmetro
erros <- sapply(resultados, function(modelo) modelo$err.rate[1])  # Extrair as taxas de erro de cada modelo
melhor_modelo <- resultados[[which.min(erros)]]                      # Extrair o melhor modelo
melhor_modelo
## Criando Modelo com melhor Configuração de Hiperparâmetro
modelo_v1 <- randomForest(
formula = Target ~ .,                         # Definir sua fórmula aqui
data = dados_treino,                          # Seus dados de treinamento
ntree = melhor_modelo$ntree,                  # Número de árvores na floresta
mtry = melhor_modelo$mtry,                    # Número de variáveis a serem consideradas em cada divisão
min.node.size = melhor_modelo$min.node.size,  # Tamanho mínimo do nó
importance = T
)
modelo_v1
## Previsões
y_pred_v1 <- predict(modelo_v1, newdata = dados_teste)
# Previsões de Probabilidade
y_pred_proba_v1 <- predict(modelo_v1, newdata = dados_teste, type = "prob")
## Avaliação do Modelo
conf_matrix_v1 <- confusionMatrix(y_pred_v1, dados_teste$Target)
roc_obj_v1 <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1[, "Class1"]))
roc_auc_v1 <- auc(roc_obj_v1)
acuracia_v1 <- sum(y_pred_v1 == dados_teste$Target) / length(y_pred_v1)
# Salvando as métricas do modelo_v1 em um Dicionário
dict_modelo_v1 <- data.frame(
Nome = "modelo_v1",
Algoritmo = "Random Forest",
ROC_AUC_Score = as.numeric(roc_auc_v1),
AUC_Score = as.numeric(auc(roc_obj_v1)),
Acuracia = acuracia_v1
)
# Adiciona o Dicionário com resultado das métricas do modelo_v1 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v1)
df_modelos
rm(tuned_params_v2, resultados, n, split, leaf, modelo, erros, y_pred_v1, y_pred_proba_v1, conf_matrix_v1,
roc_obj_v1, roc_auc_v1, acuracia_v1, dict_modelo_v1, modelo_v1)
## Versão 2
# - Aplica técnica de Feature Selection usando configurações de hiperparâmetros de modelo_v1
## Criando Modelo Para Seleção de Variáveis (Feature Selection)
modelo <- randomForest(Target ~ .,
data = dados_treino,
ntree = 200, nodesize = 10, importance = T)
# Visualizando por números
print(modelo$importance)
# Visualizando por Gráficos
varImpPlot(modelo)
importancia_ordenada <- modelo$importance[order(-modelo$importance[, 1]), , drop = FALSE]
df_importancia <- data.frame(
Variavel = rownames(importancia_ordenada),
Importancia = importancia_ordenada[, 1]
)
ggplot(df_importancia, aes(x = reorder(Variavel, -Importancia), y = Importancia)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Importância das Variáveis", x = "Variável", y = "Importância") +
theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10))
## Recriando Modelo Usando Feature Selection
# Selecionando as 5 variáveis mais importantes
vars_importantes <- head(df_importancia$Variavel, 5)
# Criando novos conjuntos de dados de treino e teste apenas com as variáveis selecionadas
dados_treino_importantes <- dados_treino[, c(vars_importantes, "Target")]
dados_teste_importantes <- dados_teste[, c(vars_importantes, "Target")]
# Recriando o modelo com as variáveis selecionadas
modelo_v2 <- randomForest(
formula = Target ~ .,                           # Fórmula usando todas as variáveis disponíveis no novo conjunto de dados
data = dados_treino_importantes,                # Novo conjunto de dados de treinamento
ntree = melhor_modelo$ntree,                    # Número de árvores na floresta do melhor modelo
mtry = melhor_modelo$mtry,                      # Número de variáveis consideradas para cada divisão no melhor modelo
nodesize = melhor_modelo$min.node.size          # Tamanho mínimo dos nós do melhor modelo
)
modelo_v2
## Previsões
y_pred_v2 <- predict(modelo_v2, newdata = dados_teste_importantes)
y_pred_proba_v2 <- predict(modelo_v2, newdata = dados_teste_importantes, type = "prob")[,2]  # Probabilidades para a classe positiva
## Matriz de confusão e métricas de avaliação
conf_matrix_v2 <- confusionMatrix(y_pred_v2, dados_teste_importantes$Target)
roc_auc_v2 <- roc(response = dados_teste_importantes$Target, predictor = y_pred_proba_v2)
auc_v2 <- auc(roc_auc_v2)
acuracia_v2 <- conf_matrix_v2$overall['Accuracy']
# Salvando Resultados
dict_modelo_v2 <- data.frame(
Nome = 'modelo_v2',
Algoritmo = 'Random Forest com Variáveis Selecionadas',
ROC_AUC_Score = roc_auc_v2$auc,
AUC_Score = auc_v2,
Acuracia = acuracia_v2
)
# Concatenando com outros dataframe com todos os resultados
row.names(dict_modelo_v2) <- NULL
df_modelos <- rbind(df_modelos, dict_modelo_v2)
df_modelos
rm(modelo, importancia_ordenada, df_importancia, dict_modelo_v2, modelo_v2, vars_importantes, dados_treino_importantes, dados_teste_importantes,
y_pred_v2, y_pred_proba_v2, conf_matrix_v2, roc_auc_v2, auc_v2, acuracia_v2, melhor_modelo)
###  Modelo 3 com KNN
# - Vamos tentar agora um algoritmo mais simples, o KNN.
# - Para esse algoritmo precisamos antes definir o valor de K, que é o número de vizinhos mais próximos.
## Versão 1
# - Com o algoritmo KNN não extraímos as variáveis mais importantes, pois o conceito do algoritmo é diferente.
# - Será criado apenas 1 versão.
## Preparando Dados de Treino e Teste
X_treino <- dados_treino[, -ncol(dados_treino)]
y_treino <- dados_treino[, ncol(dados_treino)]
X_teste <- dados_teste[, -ncol(dados_teste)]
y_teste <- dados_teste[, ncol(dados_teste)]
# Lista de possíveis valores de K
vizinhos <- seq(1, 19, by = 2)
# Definindo o controle da validação cruzada
controle <- trainControl(method = "cv", number = 5)
# Treinando o modelo KNN para vários valores de K
modelo_knn <- train(x = X_treino, y = y_treino, method = "knn",
tuneGrid = data.frame(.k = vizinhos),
trControl = controle,
metric = "Accuracy")
modelo_knn
# Extrair o melhor valor de K
optimal_k <- modelo_knn$results$k[which.max(modelo_knn$results$Accuracy)]
cat('O valor ideal de k usado no modelo foi:', optimal_k, '\n')
## Previsões
y_pred_v3 <- predict(modelo_knn, newdata = X_teste)
y_pred_proba_v3 <- predict(modelo_knn, newdata = X_teste, type = "prob")[,2]  # Probabilidades para a classe positiva
## Avaliação do Modelo
conf_matrix_v3 <- confusionMatrix(y_pred_v3, y_teste)
roc_auc_v3 <- roc(response = y_teste, predictor = y_pred_proba_v3)
auc_v3 <- auc(roc_auc_v3)
acuracia_v3 <- conf_matrix_v3$overall['Accuracy']
## Salvando Resultados
dict_modelo_v3 <- data.frame(
Nome = 'modelo_v1',
Algoritmo = 'KNN com K ótimo',
ROC_AUC_Score = roc_auc_v3$auc,
AUC_Score = auc_v3,
Acuracia = acuracia_v3
)
# Concatenando com outro DataFrame que contém todos os resultados
row.names(dict_modelo_v3) <- NULL
df_modelos <- rbind(df_modelos, dict_modelo_v3)
df_modelos
rm(X_treino, y_treino, X_teste, y_teste, vizinhos, controle, modelo_knn, optimal_k, y_pred_v3, y_pred_proba_v3,
conf_matrix_v3, roc_auc_v3, auc_v3, acuracia_v3, dict_modelo_v3)
###  Modelo 4 com Decision Tree
# - Na versão 4 do modelo usaremos um modelo de árvore de decisão.
## Versão 1
# Preparação dos dados
X_treino <- dados_treino[, -ncol(dados_treino)]
y_treino <- dados_treino$Target
X_teste <- dados_teste[, -ncol(dados_teste)]
y_teste <- dados_teste$Target
# Definindo os hiperparâmetros
tuned_params_DT <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
# Definindo o controle do treino
train_control <- trainControl(method = "cv", number = 10, search = "random", classProbs = TRUE, summaryFunction = twoClassSummary)
# Treinando o modelo com RandomizedSearchCV
set.seed(123)
modelo_DT <- train(x = X_treino, y = y_treino,
method = "rpart",
trControl = train_control,
tuneGrid = tuned_params_DT,
metric = "ROC",
maximize = TRUE)
modelo_DT
# Extraindo os melhores hiperparâmetros
best_params <- modelo_DT$bestTune
best_params
# Treinando o modelo final com os melhores hiperparâmetros
modelo_final_DT <- rpart(Target ~ ., data = dados_treino,
control = rpart.control(cp = best_params$cp))
## Previsões
# Previsões com dados de teste
y_pred_v1_DT <- predict(modelo_final_DT, X_teste, type = "class")
print('Previsões de Classe')
print(head(y_pred_v1_DT, 10))
# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_v1_DT <- predict(modelo_final_DT, X_teste, type = "prob")
print('Previsões de Probabilidade')
print(head(y_pred_proba_v1_DT, 10))
# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva
y_pred_proba_v1_DT_pos <- y_pred_proba_v1_DT[, "Class1"]
print('Previsões de Probabilidade para a Classe Positiva')
print(head(y_pred_proba_v1_DT_pos, 10))
## Avaliação do Modelo
# Matriz de confusão
conf_matrix <- confusionMatrix(y_pred_v1_DT, y_teste)
print(conf_matrix)
# Calculando a AUC
roc_auc_v1_DT <- roc(y_teste, y_pred_proba_v1_DT_pos, levels = rev(levels(y_teste)))
auc_v1_DT <- auc(roc_auc_v1_DT)
print(paste("AUC:", auc_v1_DT))
# Acurácia em teste
acuracia_v1_DT <- conf_matrix$overall["Accuracy"]
print(paste("Acurácia:", acuracia_v1_DT))
# Curva ROC
roc_curve <- roc(y_teste, y_pred_proba_v1_DT_pos)
plot(roc_curve, col = "blue", lwd = 2, main = "Curva ROC")
# Adiciona o Resultado das Métricas a uma Lista
dict_modelo_v1_DT <- list(Nome = "modelo_v1_DT",
Algoritmo = "Decision Tree",
ROC_AUC_Score = roc_auc_v1_DT$auc,
AUC_Score = auc_v1_DT,
Acuracia = acuracia_v1_DT)
print(dict_modelo_v1_DT)
# Concatenando com outros dataframe com todos os resultados
row.names(dict_modelo_v1_DT) <- NULL
df_modelos <- rbind(df_modelos, dict_modelo_v1_DT)
df_modelos
## Versão 2
# - Utiliza a técnica de Feature Selection
# Verificando a importância das variáveis
importancia <- varImp(modelo_DT, scale = FALSE)
print(importancia)
# Gráfico de importância das variáveis
plot(importancia, main = "Importância das Variáveis")
# Extraindo os nomes das 5 variáveis mais importantes
top5_vars <- rownames(importancia$importance)[order(importancia$importance$Overall, decreasing = TRUE)[1:5]]
print(top5_vars)
# Recriando os dados de treino e teste com as 5 variáveis mais importantes
X_treino_top5 <- X_treino[, top5_vars]
X_teste_top5 <- X_teste[, top5_vars]
# Treinando o novo modelo com as 5 variáveis mais importantes
set.seed(123)
modelo_DT_top5 <- train(x = X_treino_top5, y = y_treino,
method = "rpart",
trControl = train_control,
tuneGrid = tuned_params_DT,
metric = "ROC",
maximize = TRUE)
# Extraindo os melhores hiperparâmetros para o novo modelo
best_params_top5 <- modelo_DT_top5$bestTune
# Treinando o modelo final com os melhores hiperparâmetros
modelo_final_DT_top5 <- rpart(Target ~ ., data = cbind(X_treino_top5, Target = y_treino),
control = rpart.control(cp = best_params_top5$cp))
## Previsões
# Previsões com dados de teste
y_pred_v2_DT <- predict(modelo_final_DT_top5, X_teste_top5, type = "class")
print('Previsões de Classe')
print(head(y_pred_v2_DT, 10))
# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_v2_DT <- predict(modelo_final_DT_top5, X_teste_top5, type = "prob")
print('Previsões de Probabilidade')
print(head(y_pred_proba_v2_DT, 10))
# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva
y_pred_proba_v2_DT_pos <- y_pred_proba_v2_DT[, "Class1"]
print('Previsões de Probabilidade para a Classe Positiva')
print(head(y_pred_proba_v2_DT_pos, 10))
## Avaliação do Modelo
# Matriz de confusão
conf_matrix_v2 <- confusionMatrix(y_pred_v2_DT, y_teste)
print(conf_matrix_v2)
# Calculando a AUC
roc_auc_v2_DT <- roc(y_teste, y_pred_proba_v2_DT_pos, levels = rev(levels(y_teste)))
auc_v2_DT <- auc(roc_auc_v2_DT)
print(paste("AUC:", auc_v2_DT))
# Acurácia em teste
acuracia_v2_DT <- conf_matrix_v2$overall["Accuracy"]
print(paste("Acurácia:", acuracia_v2_DT))
# Curva ROC
roc_curve_v2 <- roc(y_teste, y_pred_proba_v2_DT_pos)
plot(roc_curve_v2, col = "blue", lwd = 2, main = "Curva ROC")
# Adiciona o Resultado das Métricas a uma Lista
dict_modelo_v2_DT <- list(Nome = "modelo_v2_DT",
Algoritmo = "Decision Tree com Variáveis Selecionadas",
ROC_AUC_Score = roc_auc_v2_DT$auc,
AUC_Score = auc_v2_DT,
Acuracia = acuracia_v2_DT)
print(dict_modelo_v2_DT)
# Concatenando com outros dataframe com todos os resultados
row.names(dict_modelo_v2_DT) <- NULL
df_modelos <- rbind(df_modelos, dict_modelo_v2_DT)
df_modelos
rm(X_treino, y_treino, X_teste, y_teste, tuned_params_DT, train_control, best_params, modelo_final_DT, y_pred_v1_DT,
y_pred_proba_v1_DT, conf_matrix, roc_auc_v1_DT, auc_v1_DT, acuracia_v1_DT, roc_curve, dict_modelo_v1_DT, importancia, modelo_DT,
top5_vars, X_treino_top5, X_teste_top5, modelo_DT_top5, best_params_top5, modelo_final_DT_top5, y_pred_v2_DT, y_pred_proba_v2_DT,
y_pred_proba_v2_DT_pos, y_pred_proba_v1_DT_pos, conf_matrix_v2, roc_auc_v2_DT, auc_v2_DT, acuracia_v2_DT, roc_curve_v2, dict_modelo_v2_DT)
View(df_modelos)
library(e1071)
# Preparação dos dados
X_treino <- dados_treino[, -ncol(dados_treino)]
y_treino <- dados_treino$Target
X_teste <- dados_teste[, -ncol(dados_teste)]
y_teste <- dados_teste$Target
# Definindo os hiperparâmetros
tuned_params_SVM <- expand.grid(C = c(0.001, 0.01, 0.1, 1, 10),
gamma = c(0.001, 0.01, 0.1, 1))
# Definindo o controle do treino
train_control <- trainControl(method = "cv", number = 5, search = "random", classProbs = TRUE, summaryFunction = twoClassSummary)
# Treinando o modelo com RandomizedSearchCV
set.seed(123)
modelo_SVM <- train(x = X_treino, y = y_treino,
method = "svmRadial",
trControl = train_control,
tuneGrid = tuned_params_SVM,
metric = "ROC",
maximize = TRUE)
# Definindo os hiperparâmetros
tuned_params_SVM <- expand.grid(C = c(0.001, 0.01, 0.1, 1, 10),
sigma = c(0.001, 0.01, 0.1, 1))
# Definindo o controle do treino
train_control <- trainControl(method = "cv", number = 5, search = "random", classProbs = TRUE, summaryFunction = twoClassSummary)
modelo_SVM <- train(x = X_treino, y = y_treino,
method = "svmRadial",
trControl = train_control,
tuneGrid = tuned_params_SVM,
metric = "ROC",
maximize = TRUE)
modelo_SVM
# Extraindo os melhores hiperparâmetros
best_params <- modelo_SVM$bestTune
print(best_params)
# Treinando o modelo final com os melhores hiperparâmetros
modelo_final_SVM <- svm(x = X_treino, y = y_treino,
type = "C-classification",
kernel = "radial",
cost = best_params$C,
gamma = best_params$sigma,  # Note que aqui utilizamos 'sigma' que é equivalente a 'gamma'
probability = TRUE)
modelo_final_SVM
# Previsões com dados de teste
y_pred_SVM <- predict(modelo_final_SVM, X_teste)
print('Previsões de Classe')
print(head(y_pred_SVM, 10))
# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_SVM <- attr(predict(modelo_final_SVM, X_teste, probability = TRUE), "probabilities")
print('Previsões de Probabilidade')
print(head(y_pred_proba_SVM, 10))
# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva
y_pred_proba_SVM_pos <- y_pred_proba_SVM[, "Class1"]
print('Previsões de Probabilidade para a Classe Positiva')
print(head(y_pred_proba_SVM_pos, 10))
# Matriz de confusão
conf_matrix_SVM <- confusionMatrix(y_pred_SVM, y_teste)
print(conf_matrix_SVM)
# Calculando a AUC
roc_auc_SVM <- roc(y_teste, y_pred_proba_SVM_pos, levels = rev(levels(y_teste)))
auc_SVM <- auc(roc_auc_SVM)
print(paste("AUC:", auc_SVM))
# Acurácia em teste
acuracia_SVM <- conf_matrix_SVM$overall["Accuracy"]
print(paste("Acurácia:", acuracia_SVM))
# Curva ROC
roc_curve_SVM <- roc(y_teste, y_pred_proba_SVM_pos)
plot(roc_curve_SVM, col = "blue", lwd = 2, main = "Curva ROC")
# Adiciona o Resultado das Métricas a uma Lista
dict_modelo_SVM <- list(Nome = "modelo_SVM",
Algoritmo = "SVM",
ROC_AUC_Score = roc_auc_SVM$auc,
AUC_Score = auc_SVM,
Acurácia = acuracia_SVM)
print(dict_modelo_SVM)
# Concatenando com outros dataframe com todos os resultados
row.names(dict_modelo_SVM) <- NULL
df_modelos <- rbind(df_modelos, dict_modelo_SVM)
df_modelos <- rbind(df_modelos, dict_modelo_SVM)
# Adiciona o Resultado das Métricas a uma Lista
dict_modelo_SVM <- list(Nome = "modelo_SVM",
Algoritmo = "SVM",
ROC_AUC_Score = roc_auc_SVM$auc,
AUC_Score = auc_SVM,
Acuracia = acuracia_SVM)
print(dict_modelo_SVM)
# Concatenando com outros dataframe com todos os resultados
row.names(dict_modelo_SVM) <- NULL
df_modelos <- rbind(df_modelos, dict_modelo_SVM)
df_modelos
# Identificação das variáveis mais importantes usando RFE
control <- rfeControl(functions = caretFuncs, method = "cv", number = 5)
results <- rfe(X_treino, y_treino, sizes = c(1:5), rfeControl = control)
