## Verificando e Tratando Valores Outliers
# - Irá ser apresentado dois cenários e tomaremos decisões diferentes para cada um deles.
## Cenário 1 (Variável 'Alamine_Aminotransferase')
# Sumário
summary(df$Alamine_Aminotransferase)
# - Através do sumário, podemos observar que a variável possui uma média de 80.14 e um valor máx de 2000. Isso é um sinal de que podemos ter um outlier.
# - Podemos checar esta informação através da criação de um Gráfico BoxPlot.
# Gráfico BoxPLot
ggplot(df, aes(y = Alamine_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Alamine Aminotransferase", y = "Alamine Aminotransferase") +
theme_minimal()  # Aplicando um tema minimalista
# Interpretando o Gráfico
# - Podemos verificar que além do valor de 2000 temos outros diversos valores acima da média (que está próxima de zero).
# - Será que os valores extremos são mesmo outliers para esta variável?
# Podemos responder isso verificando contagem de frequência por valor abaixo (filtrando os 5 maiores valores):
# Exibindo os cinco maiores valores únicos e suas frequências:
table(df$Alamine_Aminotransferase)[as.character(sort(unique(df$Alamine_Aminotransferase), decreasing = TRUE)[1:5])]
# Exibindo a quantidade de valores acima da média:
sum(df$Alamine_Aminotransferase > mean(df$Alamine_Aminotransferase))  # Contagem de valores acima da média
length(df$Alamine_Aminotransferase)                                   # Contagem total de valores da variável
# Conclusão
# - Após a análise detalhada da variável 'Alamine_Aminotransferase', identificamos que o valor máximo de 2000 é consideravelmente mais alto que os outros
#   valores próximos, que também são altos mas menos frequentes.
# - Esses valores extremos podem ser considerados outliers devido ao seu afastamento significativo da média e mediana, além de serem raros no dataset,
#   como mostrado pela análise de frequência.
# - Dado esse contexto, é sugerido a avaliação de tratamento desses outliers dentro do cenário de aplicação dos dados. Se estes valores são resultantes de
#   erros de medição ou casos muito atípicos que podem distorcer análises estatísticas, a remoção ou substituição por um limite superior calculado pelo
#   método do IQR é recomendada.
# - Contudo, se esses altos valores representam casos válidos dentro da pesquisa ou aplicação prática dos dados, poderiam ser mantidos, mas com uma análise
#   adicional para confirmar sua validade.
# - Portanto neste caso específico, após verificar a validade dos dados, optou-se por não realizar o tratamento de outliers para esta variável, pois eles
#   representam casos autênticos dentro do contexto estudado.
## Cenário 2 (Variável 'Aspartate_Aminotransferase')
# Sumário
summary(df$Aspartate_Aminotransferase)
# - Através do Sumário podemos observar que a variável possui uma média de 109.89 e um valor máx de 4929. Isso é um sinal de que podemos ter um ou mais
#   valores outlier.
# - Vamos novamente verificar por um Gráfico BoxPlot.
# Gráfico BoxPLot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Aspartate Aminotransferase", y = "Aspartate Aminotransferase") +
theme_minimal()  # Aplicando um tema minimalista
# Interpretando o gráfico
# - Podemos verificar que novamente temos valores outliers, mas com um comportamente diferente. Parece que temos menos dados com valores extremos.
# - Aqui nós temos apenas dois valores outliers acima de 2000 enquanto todos os outros abaixo deste valor.
# - E neste caso, todos esses valores extremos são mesmo outliers para esta variável?
# Podemos responder isso verificando novamente os maiores valores únicos e suas frequências:
# Exibindo os cinco maiores valores únicos e suas frequências:
table(df$Aspartate_Aminotransferase)[as.character(sort(unique(df$Aspartate_Aminotransferase), decreasing = TRUE)[1:5])]
# Exibindo a quantidade de valores acima da média:
sum(df$Aspartate_Aminotransferase > mean(df$Aspartate_Aminotransferase))  # Contagem de valores acima da média
length(df$Aspartate_Aminotransferase)                                     # Contagem total de valores da variável
# Exibindo a quantidade de valores acima de 2000:
sum(df$Aspartate_Aminotransferase > 2000)                                 # Contagem de valores acima de 2000
length(df$Aspartate_Aminotransferase)                                     # Contagem total de valores da variável
# Conclusão
# - Vamos aplicar um tratamento para limpeza de outlier nesta variável.
# - Iremos manter no dataset todos os registros abaixo do valor 2500 para esta variável.
# Tratando Valores Outliers da Variável 'Alamine_Aminotransferase'
dim(df)
summary(df)
# Aplica tratamento mantendo somente os registros onde o valor for menor ou igual a 3000 e verifica shape
df <- df %>% filter(Aspartate_Aminotransferase <= 3000)
dim(df)
# BoxPlot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Aspartate Aminotransferase Após Primeiro Filtro", y = "Aspartate Aminotransferase")
# Aplica novo tratamento mantendo somente os registros onde o valor for menor ou igual a 2500 e verifica shape
df <- df %>% filter(Aspartate_Aminotransferase <= 2500)
# BoxPlot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Aspartate Aminotransferase Após Segundo Filtro", y = "Aspartate Aminotransferase")
dim(df)
summary(df)
## Tratando Valores Ausentes
dim(df)
# Removendo todas linhas com valores ausentes
df <- df %>% drop_na()
dim(df)
#### RESUMO
# - Antes de avançarmos para a etapa final de pré-processamento de dados, crucial para a construção de modelos de machine learning, vamos recapitular
#   os passos já concluídos no projeto.:
#  -> Primeiro foi definido o problema de negócio para saber o objetivo e o que temos que resolver.
#  -> Depois nós extraímos os dados e nesta etapa pode ser que tenhamos o suporte de um Engenheiro de Dados. No caso deste projeto foi feito a leitura dos
#     dados através de um arquivo csv.
#  -> Na sequência foi feita a Análise Exploratória onde nós verificamos padrões, detectamos problemas, identifica coisas que precisamos fazer.
#  -> Após isso é aplicado a Limpeza de Dados de acordo com as técnicas necessárias, estratégias e decisões.
#  -> Sempre lembrar de documentar tudo o que foi feito em cada atividade.
#### Pré-Processamento de Dados Para Construção de Modelos de Machine Learning
# - Como vimos anteriormente ao aplicarmos o mapa de correlação as variáveis 'Direct_Bilirubin' e 'Total_Bilirubin' possuem uma alta correlação.
# - Com isso foi tomada a decisão de remover umas das variáveis.
# Removendo Variável 'Direct_Bilirubin'
df <- df %>%
select(-Direct_Bilirubin)
##  Dividindo os dados em treino e teste
set.seed(100)
indices <- createDataPartition(df$Target, p = 0.75, list = FALSE)
dados_treino <- df[indices, ]
dados_teste <- df[-indices, ]
rm(indices)
## Balanceamento de Classes
# Contagem
table(dados_treino$Target)
# Por que realizar o Balanceamento de Classes ?
# - Como foi observado no table() acima podemos constatar que os dados estão desbalanceados, isso signifca que tem muito mais pacientes de uma classe do
#   que da outra.
# - E o que acontece quando não realizarmos o balanceamento? O modelo de ML aprenderá muito mais o padrão da Classe 1 do que da Classe 0.
# - Caso não aplicamos técnica de Balanceamento, o modelo tende a ficar tendencioso. Por isso precisamos fazer o Balanceamento de Classes.
# Estratégias para o Balanceamento
# Temos duas estratégias:
# - Reduzir os registros da classe majoritária e assim diminuir consideravelmente o número de registros no nosso dataset.
# - Aplicar a técnica de Oversampling onde irá ser aumentado o número de registros das classe minoritária. E como isso é feito? Sendo criado dados
#   sintéticos com base nos dados existentes (Para isso, podemos utilizar o pacote ROSE em R, que oferece funções para gerar dados sintéticos).
# Balanceamento da Variável Alvo (Aplicando a técnica Oversampling para balancear a variável alvo)
table(dados_treino$Target)
dados_balanceados <- ovun.sample(Target ~ ., data = dados_treino, method = "over", N = 2*max(table(dados_treino$Target)))$data
table(dados_balanceados$Target)
# Por que a técnica de oversamping dever se aplicada somente nos dados de treino?
# - A técnica de oversampling deve ser aplicada somente nos dados de treino para evitar o vazamento de dados (data leakage) e garantir uma avaliação justa
#   e realista do modelo durante o teste.
# - Se o balanceamento fosse aplicado ao conjunto de dados completo, incluindo os dados de teste, o modelo poderia acabar sendo avaliado com dados
#   sintéticos, não representativos da realidade, influenciando os resultados dos testes e comprometendo a capacidade de generalização do modelo para novos
#   dados não vistos.
# - Portanto, mantendo o conjunto de teste original, sem dados sintéticos, asseguramos que a performance do modelo reflete melhor sua eficácia em cenários
#   reais.
# Tamanho
dim(dados_treino)
dim(dados_balanceados)
# O dataset de treino agora passou de 423 linhas para 608 linhas.
# Ajusta o nome do dataset de treino
dados_treino <- dados_balanceados
rm(dados_balanceados)
# Contagem
table(dados_treino$Target)
## Padronização x Normalização
# As técnicas de padronização e normalização são usadas no pré-processamento de dados em aprendizado de máquina para preparar variáveis numéricas,
# ajustando suas escalas. Aqui está quando e por que usar cada uma:
## Padronização
# Transforma os dados de modo que eles tenham média zero e desvio padrão igual a um.
# - Quando usar    : Aplicável quando os dados já estão centralizados em torno de uma média e precisam de ajuste na escala. É útil em modelos como SVM e
#                    Regressão Logística, que são sensíveis a variações na escala das variáveis de entrada.
# - Exemplo prático: Se medimos altura em centímetros (150-190 cm) e peso em quilogramas (50-100 kg), a padronização permite comparar essas medidas numa
#                    escala comum, evitando distorções devido a diferentes intervalos de valores.
# - Motivo para
#   este projeto   : Optamos pela padronização porque as variáveis têm escalas muito diferentes e há a presença de outliers significativos. A padronização
#                    mantém as propriedades estatísticas dos dados, minimizando o impacto dos outliers, ao contrário da normalização que pode distorcer os
#                    dados ao comprimir a maioria dos valores em um intervalo estreito.
## Normalização
# Ajusta os dados para que seus valores caibam em um intervalo predefinido, geralmente de 0 a 1.
# - Quando usar    : Ideal para dados com variações extremas nas escalas e onde os algoritmos são sensíveis à magnitude absoluta dos dados, como
#                    K-Nearest Neighbors (KNN) e técnicas de clustering.
# - Exemplo prático: Se um dataset contém preços de produtos variando de R 1𝑎𝑅1000 e quantidades vendidas de 1 a 20 unidades, a normalização faria com
#                    que ambos os atributos tivessem a mesma contribuição no modelo, independentemente da escala original.
# - Motivo para não
#   esta no projeto: Não foi escolhida devido à presença de outliers, que poderiam ser enfatizados indevidamente, e porque a normalização poderia limitar
#                    a eficácia de modelos que assumem uma distribuição normal dos dados.
## Importante:
# - Não é necessário aplicar padronização/normalização na variável alvo.
# - Nós não aplicamos as duas técnicas, ou usamos uma ou outra.
# - A normalização pode não ser a melhor escolha se houver outliers significativos no conjunto de dados, pois isso poderia comprimir a maioria dos dados
#   em um intervalo muito estreito. Nesses casos, a padronização é recomendada.
# Padronizado Dados de Treino
summary(dados_treino)
# Calculando a média e o desvio padrão dos dados de treino
treino_mean <- sapply(dados_treino[, -which(names(dados_treino) == "Target")], mean, na.rm = TRUE)
treino_std <- sapply(dados_treino[, -which(names(dados_treino) == "Target")], sd, na.rm = TRUE)
# Exibindo a média e o desvio padrão
print(treino_mean)
print(treino_std)
# Padronizando todas as variáveis, exceto 'Target'
dados_treino[, names(treino_mean)] <- sweep(dados_treino[, names(treino_mean)], 2, treino_mean, "-")
dados_treino[, names(treino_std)] <- sweep(dados_treino[, names(treino_std)], 2, treino_std, "/")
summary(dados_treino)
# Padronizado Dados de Teste
summary(dados_teste)
# Padronizando os dados de teste usando a média e desvio padrão dos dados de treino
dados_teste[, names(treino_mean)] <- sweep(dados_teste[, names(treino_mean)], 2, treino_mean, "-")
dados_teste[, names(treino_std)] <- sweep(dados_teste[, names(treino_std)], 2, treino_std, "/")
summary(dados_teste)
rm(treino_mean, treino_std)
#### Construindo Modelos de Machine Learning
# Nesta etapa do projeto, desenvolveremos e avaliaremos cinco diferentes modelos de machine learning para identificar qual deles apresenta o melhor
# desempenho para o nosso conjunto de dados.
# Cada modelo foi escolhido por suas características únicas e capacidade de lidar com problemas de classificação.
# Abaixo estão os modelos que serão implementados e testados:
# - Modelo 1: Regressão Logística - Utilizado como benchmark devido à sua simplicidade e eficácia em problemas de classificação binária.
#   Este modelo ajudará a estabelecer uma linha base para a performance que esperamos superar com técnicas mais complexas.
# - Modelo 2: Random Forest - Um modelo de ensemble que usa múltiplas árvores de decisão para melhorar a generalização. É conhecido por sua alta precisão
#   e capacidade de ranquear a importância das variáveis.
# - Modelo 3: KNN (K-Nearest Neighbors) - Um modelo baseado em instância que faz previsões com base nas labels das amostras mais próximas no espaço de
#   características. Este modelo é eficaz em casos onde a relação entre as variáveis é altamente não-linear.
# - Modelo 4: Decision Tree (Árvore de Decisão) - Uma árvore de decisão é útil por sua interpretabilidade, permitindo entender claramente quais critérios
#   o modelo está usando para tomar decisões.
# - Modelo 5: SVM (Support Vector Machine) - Ideal para problemas de classificação e regressão de margem grande. O SVM é eficiente na criação de hiperplanos
#   em um espaço multidimensional, o que o torna adequado para casos com muitas variáveis de entrada.
# Cada modelo será treinado utilizando o mesmo conjunto de dados, permitindo uma comparação justa de sua eficácia. A avaliação de cada modelo incluirá
# métricas como precisão, AUC-ROC, entre outras, dependendo das especificidades de nosso problema e dados.
## Conversão da Variável Alvo Para Tipo Factor (Certifique-se de que Target é um fator com dois níveis)
dados_treino$Target <- factor(dados_treino$Target, levels = c(0, 1))
dados_teste$Target <- factor(dados_teste$Target, levels = c(0, 1))
# Ajustar os níveis da variável alvo para serem nomes válidos de variáveis em R
levels(dados_treino$Target) <- c("Class0", "Class1")
levels(dados_teste$Target) <- c("Class0", "Class1")
## Cria um dataframe para receber as métricas de cada modelo
df_modelos <- data.frame()
### Modelo 1 com Regressão Logística (Benchmark)
# Para a primeira versão do modelo o ideal é escolher um algoritmo simples, fácil de compreender e que será usado como Benchmark.
# Obs: Como parte do processo envolve aleatoriedade, os resultados podem ser ligeiramente diferentes a cada execução deste bloco de código.
## Versão 1
# - Cria vários modelos utilizando o pacote `caret` com um grid de hiperparâmetros para `glmnet`, que combina standardização de dados e regressão logística
#   com penalidade L2. O objetivo é encontrar os melhores hiperparâmetros.
# - O `GridSearchCV` é simulado no R com a função `train`, aplicando validação cruzada e pré-processamento. Após identificar o melhor parâmetro (lambda),
#   treina-se o modelo final diretamente com a função `glmnet` utilizando os hiperparâmetros otimizados.
# - Isso é feito para assegurar um modelo eficiente e pronto para implementações práticas.
str(dados_treino)
str(dados_teste)
# Definindo o controle de treinamento para Grid Search
train_control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# Define a lista de hiperparâmetros
lambda_grid <- 1 / c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000)
tuned_params <- expand.grid(alpha = 0, lambda = lambda_grid)
# Executa o Grid Search usando a fórmula diretamente
grid_search <- train(
Target ~ .,
data = dados_treino,
method = "glmnet",
tuneGrid = tuned_params,
trControl = train_control,
metric = "ROC",
preProcess = "scale",
family = "binomial"
)
# Extraindo os melhores parâmetros
best_lambda <- grid_search$bestTune$lambda
best_lambda
# Treinando o modelo final com os melhores parâmetros encontrados
modelo_v1 <- glmnet(
x = as.matrix(dados_treino[, -which(names(dados_treino) == "Target")]),
y = as.numeric(dados_treino$Target),
alpha = 0,  # L2 penalidade como em LogisticRegression com 'l2'
lambda = best_lambda,
family = "binomial",
standardize = TRUE  # Equivalente ao StandardScaler
)
modelo_v1
rm(train_control, lambda_grid, tuned_params, grid_search)
## Previsões
# Preparando dados de teste para previsão
X_teste <- as.matrix(dados_teste[, -which(names(dados_teste) == "Target")])
# Previsões de Classe
y_pred_v1 <- predict(modelo_v1, newx = X_teste, s = "lambda.min", type = "class")
y_pred_v1 <- as.factor(ifelse(y_pred_v1 == "1", "Class0", "Class1"))
head(y_pred_v1)
# Previsões de Probabilidade
y_pred_proba_v1 <- predict(modelo_v1, newx = X_teste, s = "lambda.min", type = "response")
head(y_pred_proba_v1)
## Avaliação do Modelo
# Matriz de Confusão
conf_matrix <- confusionMatrix(y_pred_v1, dados_teste$Target)
conf_matrix
# Calcula e exibe a métrica AUC-ROC
roc_obj <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1))
roc_auc_v1 <- auc(roc_obj)
roc_auc_v1
# Calcula a curva ROC
roc_curve <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1))
roc_curve
plot(roc_curve, main="ROC Curve", col="#1c61b6") # Opcional: Gráfico da curva ROC
# Calcula e exibe a acurácia
acuracia_v1 <- sum(y_pred_v1 == dados_teste$Target) / length(y_pred_v1)
acuracia_v1
# Exibindo os resultados
cat("Confusion Matrix:\n")
print(conf_matrix$table)
cat(sprintf("\nAUC-ROC: %f\n", roc_auc_v1))
cat(sprintf("Accuracy: %f\n", acuracia_v1))
## Salvando as métricas do modelo_v1 em um Dicionário
dict_modelo_v1 <- data.frame(
Nome = "modelo_v1",
Algoritmo = "Regressão Logística",
ROC_AUC_Score = as.numeric(roc_auc_v1),  # Converte AUC para numérico
AUC_Score = as.numeric(auc(roc_curve)),  # AUC calculada da curva ROC, também convertida
Acuracia = acuracia_v1
)
## Adiciona o Dicionário com resultado das métricas do modelo_v1 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v1)
df_modelos
rm(X_teste, y_pred_v1, y_pred_proba_v1, conf_matrix, roc_obj, acuracia_v1, roc_auc_v1, roc_curve, dict_modelo_v1)
## Versão 2
# - Aplica a Técnica de Feature Selection no modelo_v1 criado na Versão 1
# - Re-cria o modelo utilizando as 5 variáveis mais importantes
# Extraindo coeficientes do modelo para o melhor lambda
coeficientes <- as.matrix(coef(modelo_v1, s = best_lambda))
rownames(coeficientes) <- c("(Intercept)", names(dados_treino)[-which(names(dados_treino) == "Target")])
# Calculando a importância como o valor absoluto dos coeficientes
importancias <- abs(coeficientes[-1, , drop = FALSE])  # Exclui o intercepto
df_importancias <- data.frame(
Feature = rownames(importancias),
Importance = importancias[, 1]
)
df_importancias <- df_importancias[order(-df_importancias$Importance), ]
# Visualizando por Números
print(df_importancias)
# Visualiando por Gráfico
ggplot(df_importancias, aes(x = Importance, y = reorder(Feature, Importance))) +
geom_bar(stat = "identity", fill = "skyblue", orientation = "y") +
labs(title = "Importância das Variáveis", x = "Importância", y = "Variável") +
theme_minimal() +
theme(axis.text.y = element_text(angle = 0, hjust = 1))  # Melhorando a legibilidade dos rótulos
## Selecionando as 5 variáveis mais importantes
# Critério: Foi detectado uma disparidade entre as 5 primeiras e as outras 4 variáveis. Vamos escolher as 5 primerias.
vars_importantes <- head(df_importancias$Feature, 5)
## Recriando o modelo usando apenas as variáveis mais importantes
dados_treino_importantes <- dados_treino[, c(vars_importantes, "Target")]
# Recriando modelo_v2 com variáveis selecionadas
modelo_v2 <- glmnet(
x = as.matrix(dados_treino_importantes[, -which(names(dados_treino_importantes) == "Target")]),
y = as.numeric(dados_treino_importantes$Target),
alpha = 0,  # L2 penalidade como em LogisticRegression com 'l2'
lambda = best_lambda,
family = "binomial",
standardize = TRUE
)
modelo_v2
## Preparando dados de teste para previsão com as variáveis mais importantes
dados_teste_importantes <- dados_teste[, c(vars_importantes, "Target")]
X_teste_importantes <- as.matrix(dados_teste_importantes[, -which(names(dados_teste_importantes) == "Target")])
## Previsões de Classe
y_pred_v2 <- predict(modelo_v2, newx = X_teste_importantes, s = "lambda.min", type = "class")
y_pred_v2 <- as.factor(ifelse(y_pred_v2 == "1", "Class0", "Class1"))
# Previsões de Probabilidade
y_pred_proba_v2 <- predict(modelo_v2, newx = X_teste_importantes, s = "lambda.min", type = "response")
## Avaliação do Modelo
conf_matrix_v2 <- confusionMatrix(y_pred_v2, dados_teste_importantes$Target)
roc_obj_v2 <- roc(response = dados_teste_importantes$Target, predictor = as.numeric(y_pred_proba_v2))
roc_auc_v2 <- auc(roc_obj_v2)
acuracia_v2 <- sum(y_pred_v2 == dados_teste_importantes$Target) / length(y_pred_v2)
# Salvando as métricas do modelo_v2 em um Dicionário
dict_modelo_v2 <- data.frame(
Nome = "modelo_v2",
Algoritmo = "Regressão Logística com Seleção de Variáveis",
ROC_AUC_Score = as.numeric(roc_auc_v2),
AUC_Score = as.numeric(auc(roc_obj_v2)),
Acuracia = acuracia_v2
)
# Adiciona o Dicionário com resultado das métricas do modelo_v2 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v2)
df_modelos
rm(modelo_v1, modelo_v2, dados_teste_importantes, X_teste_importantes, importancias, y_pred_v2, y_pred_proba_v2, conf_matrix_v2, roc_obj_v2,
roc_auc_v2, acuracia_v2, dict_modelo_v2, best_lambda, coeficientes, df_importancias, vars_importantes, dados_treino_importantes)
###  Modelo 2 com Random Forest
# - Nosso desafio agora é tentar obter um modelo melhor que a versão 1. Vamos tentar o algoritmo Random Forest.
## Versão 1
# - Criação e treinamento do modelo com Random Forest
set.seed(123)
# Ajustando Hiperparâmetros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o parâmetro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Verificando a estrutura de tuned_params
str(tuned_params)
# Treinamento com Randomized Search
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Treinamento com Randomized Search
attr(tuned_params, "out.attrs") <- NULL
# Treinamento com Randomized Search
attr(tuned_params, "out.attrs") <- NULL
# Verificando a estrutura de tuned_params
str(tuned_params)
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Ajustando Hiperparâmetros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o parâmetro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Renomeando a coluna mtry para "mtry"
colnames(tuned_params)[colnames(tuned_params) == "mtry"] <- "mtry"
str(tuned_params)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Ajustando Hiperparâmetros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o parâmetro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Renomeando a coluna mtry para "mtry"
attr(tuned_params, "out.attrs") <- NULL
colnames(tuned_params)[colnames(tuned_params) == "mtry"] <- "mtry"
str(tuned_params)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Ajustando Hiperparâmetros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o parâmetro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Adicionando uma coluna mtry adicional para satisfazer as expectativas do caret
tuned_params$mtry <- sample(c(2, 3, 4), size = nrow(tuned_params), replace = TRUE)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Define o grid de hiperparâmetros
tuned_params_v2 <- expand.grid(
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params_v2,
metric = "ROC"
)
