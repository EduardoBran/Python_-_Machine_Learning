## Verificando e Tratando Valores Outliers
# - Ir√° ser apresentado dois cen√°rios e tomaremos decis√µes diferentes para cada um deles.
## Cen√°rio 1 (Vari√°vel 'Alamine_Aminotransferase')
# Sum√°rio
summary(df$Alamine_Aminotransferase)
# - Atrav√©s do sum√°rio, podemos observar que a vari√°vel possui uma m√©dia de 80.14 e um valor m√°x de 2000. Isso √© um sinal de que podemos ter um outlier.
# - Podemos checar esta informa√ß√£o atrav√©s da cria√ß√£o de um Gr√°fico BoxPlot.
# Gr√°fico BoxPLot
ggplot(df, aes(y = Alamine_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Alamine Aminotransferase", y = "Alamine Aminotransferase") +
theme_minimal()  # Aplicando um tema minimalista
# Interpretando o Gr√°fico
# - Podemos verificar que al√©m do valor de 2000 temos outros diversos valores acima da m√©dia (que est√° pr√≥xima de zero).
# - Ser√° que os valores extremos s√£o mesmo outliers para esta vari√°vel?
# Podemos responder isso verificando contagem de frequ√™ncia por valor abaixo (filtrando os 5 maiores valores):
# Exibindo os cinco maiores valores √∫nicos e suas frequ√™ncias:
table(df$Alamine_Aminotransferase)[as.character(sort(unique(df$Alamine_Aminotransferase), decreasing = TRUE)[1:5])]
# Exibindo a quantidade de valores acima da m√©dia:
sum(df$Alamine_Aminotransferase > mean(df$Alamine_Aminotransferase))  # Contagem de valores acima da m√©dia
length(df$Alamine_Aminotransferase)                                   # Contagem total de valores da vari√°vel
# Conclus√£o
# - Ap√≥s a an√°lise detalhada da vari√°vel 'Alamine_Aminotransferase', identificamos que o valor m√°ximo de 2000 √© consideravelmente mais alto que os outros
#   valores pr√≥ximos, que tamb√©m s√£o altos mas menos frequentes.
# - Esses valores extremos podem ser considerados outliers devido ao seu afastamento significativo da m√©dia e mediana, al√©m de serem raros no dataset,
#   como mostrado pela an√°lise de frequ√™ncia.
# - Dado esse contexto, √© sugerido a avalia√ß√£o de tratamento desses outliers dentro do cen√°rio de aplica√ß√£o dos dados. Se estes valores s√£o resultantes de
#   erros de medi√ß√£o ou casos muito at√≠picos que podem distorcer an√°lises estat√≠sticas, a remo√ß√£o ou substitui√ß√£o por um limite superior calculado pelo
#   m√©todo do IQR √© recomendada.
# - Contudo, se esses altos valores representam casos v√°lidos dentro da pesquisa ou aplica√ß√£o pr√°tica dos dados, poderiam ser mantidos, mas com uma an√°lise
#   adicional para confirmar sua validade.
# - Portanto neste caso espec√≠fico, ap√≥s verificar a validade dos dados, optou-se por n√£o realizar o tratamento de outliers para esta vari√°vel, pois eles
#   representam casos aut√™nticos dentro do contexto estudado.
## Cen√°rio 2 (Vari√°vel 'Aspartate_Aminotransferase')
# Sum√°rio
summary(df$Aspartate_Aminotransferase)
# - Atrav√©s do Sum√°rio podemos observar que a vari√°vel possui uma m√©dia de 109.89 e um valor m√°x de 4929. Isso √© um sinal de que podemos ter um ou mais
#   valores outlier.
# - Vamos novamente verificar por um Gr√°fico BoxPlot.
# Gr√°fico BoxPLot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Aspartate Aminotransferase", y = "Aspartate Aminotransferase") +
theme_minimal()  # Aplicando um tema minimalista
# Interpretando o gr√°fico
# - Podemos verificar que novamente temos valores outliers, mas com um comportamente diferente. Parece que temos menos dados com valores extremos.
# - Aqui n√≥s temos apenas dois valores outliers acima de 2000 enquanto todos os outros abaixo deste valor.
# - E neste caso, todos esses valores extremos s√£o mesmo outliers para esta vari√°vel?
# Podemos responder isso verificando novamente os maiores valores √∫nicos e suas frequ√™ncias:
# Exibindo os cinco maiores valores √∫nicos e suas frequ√™ncias:
table(df$Aspartate_Aminotransferase)[as.character(sort(unique(df$Aspartate_Aminotransferase), decreasing = TRUE)[1:5])]
# Exibindo a quantidade de valores acima da m√©dia:
sum(df$Aspartate_Aminotransferase > mean(df$Aspartate_Aminotransferase))  # Contagem de valores acima da m√©dia
length(df$Aspartate_Aminotransferase)                                     # Contagem total de valores da vari√°vel
# Exibindo a quantidade de valores acima de 2000:
sum(df$Aspartate_Aminotransferase > 2000)                                 # Contagem de valores acima de 2000
length(df$Aspartate_Aminotransferase)                                     # Contagem total de valores da vari√°vel
# Conclus√£o
# - Vamos aplicar um tratamento para limpeza de outlier nesta vari√°vel.
# - Iremos manter no dataset todos os registros abaixo do valor 2500 para esta vari√°vel.
# Tratando Valores Outliers da Vari√°vel 'Alamine_Aminotransferase'
dim(df)
summary(df)
# Aplica tratamento mantendo somente os registros onde o valor for menor ou igual a 3000 e verifica shape
df <- df %>% filter(Aspartate_Aminotransferase <= 3000)
dim(df)
# BoxPlot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Aspartate Aminotransferase Ap√≥s Primeiro Filtro", y = "Aspartate Aminotransferase")
# Aplica novo tratamento mantendo somente os registros onde o valor for menor ou igual a 2500 e verifica shape
df <- df %>% filter(Aspartate_Aminotransferase <= 2500)
# BoxPlot
ggplot(df, aes(y = Aspartate_Aminotransferase)) +
geom_boxplot(fill = "blue", colour = "black") +
labs(title = "Boxplot de Aspartate Aminotransferase Ap√≥s Segundo Filtro", y = "Aspartate Aminotransferase")
dim(df)
summary(df)
## Tratando Valores Ausentes
dim(df)
# Removendo todas linhas com valores ausentes
df <- df %>% drop_na()
dim(df)
#### RESUMO
# - Antes de avan√ßarmos para a etapa final de pr√©-processamento de dados, crucial para a constru√ß√£o de modelos de machine learning, vamos recapitular
#   os passos j√° conclu√≠dos no projeto.:
#  -> Primeiro foi definido o problema de neg√≥cio para saber o objetivo e o que temos que resolver.
#  -> Depois n√≥s extra√≠mos os dados e nesta etapa pode ser que tenhamos o suporte de um Engenheiro de Dados. No caso deste projeto foi feito a leitura dos
#     dados atrav√©s de um arquivo csv.
#  -> Na sequ√™ncia foi feita a An√°lise Explorat√≥ria onde n√≥s verificamos padr√µes, detectamos problemas, identifica coisas que precisamos fazer.
#  -> Ap√≥s isso √© aplicado a Limpeza de Dados de acordo com as t√©cnicas necess√°rias, estrat√©gias e decis√µes.
#  -> Sempre lembrar de documentar tudo o que foi feito em cada atividade.
#### Pr√©-Processamento de Dados Para Constru√ß√£o de Modelos de Machine Learning
# - Como vimos anteriormente ao aplicarmos o mapa de correla√ß√£o as vari√°veis 'Direct_Bilirubin' e 'Total_Bilirubin' possuem uma alta correla√ß√£o.
# - Com isso foi tomada a decis√£o de remover umas das vari√°veis.
# Removendo Vari√°vel 'Direct_Bilirubin'
df <- df %>%
select(-Direct_Bilirubin)
##  Dividindo os dados em treino e teste
set.seed(100)
indices <- createDataPartition(df$Target, p = 0.75, list = FALSE)
dados_treino <- df[indices, ]
dados_teste <- df[-indices, ]
rm(indices)
## Balanceamento de Classes
# Contagem
table(dados_treino$Target)
# Por que realizar o Balanceamento de Classes ?
# - Como foi observado no table() acima podemos constatar que os dados est√£o desbalanceados, isso signifca que tem muito mais pacientes de uma classe do
#   que da outra.
# - E o que acontece quando n√£o realizarmos o balanceamento? O modelo de ML aprender√° muito mais o padr√£o da Classe 1 do que da Classe 0.
# - Caso n√£o aplicamos t√©cnica de Balanceamento, o modelo tende a ficar tendencioso. Por isso precisamos fazer o Balanceamento de Classes.
# Estrat√©gias para o Balanceamento
# Temos duas estrat√©gias:
# - Reduzir os registros da classe majorit√°ria e assim diminuir consideravelmente o n√∫mero de registros no nosso dataset.
# - Aplicar a t√©cnica de Oversampling onde ir√° ser aumentado o n√∫mero de registros das classe minorit√°ria. E como isso √© feito? Sendo criado dados
#   sint√©ticos com base nos dados existentes (Para isso, podemos utilizar o pacote ROSE em R, que oferece fun√ß√µes para gerar dados sint√©ticos).
# Balanceamento da Vari√°vel Alvo (Aplicando a t√©cnica Oversampling para balancear a vari√°vel alvo)
table(dados_treino$Target)
dados_balanceados <- ovun.sample(Target ~ ., data = dados_treino, method = "over", N = 2*max(table(dados_treino$Target)))$data
table(dados_balanceados$Target)
# Por que a t√©cnica de oversamping dever se aplicada somente nos dados de treino?
# - A t√©cnica de oversampling deve ser aplicada somente nos dados de treino para evitar o vazamento de dados (data leakage) e garantir uma avalia√ß√£o justa
#   e realista do modelo durante o teste.
# - Se o balanceamento fosse aplicado ao conjunto de dados completo, incluindo os dados de teste, o modelo poderia acabar sendo avaliado com dados
#   sint√©ticos, n√£o representativos da realidade, influenciando os resultados dos testes e comprometendo a capacidade de generaliza√ß√£o do modelo para novos
#   dados n√£o vistos.
# - Portanto, mantendo o conjunto de teste original, sem dados sint√©ticos, asseguramos que a performance do modelo reflete melhor sua efic√°cia em cen√°rios
#   reais.
# Tamanho
dim(dados_treino)
dim(dados_balanceados)
# O dataset de treino agora passou de 423 linhas para 608 linhas.
# Ajusta o nome do dataset de treino
dados_treino <- dados_balanceados
rm(dados_balanceados)
# Contagem
table(dados_treino$Target)
## Padroniza√ß√£o x Normaliza√ß√£o
# As t√©cnicas de padroniza√ß√£o e normaliza√ß√£o s√£o usadas no pr√©-processamento de dados em aprendizado de m√°quina para preparar vari√°veis num√©ricas,
# ajustando suas escalas. Aqui est√° quando e por que usar cada uma:
## Padroniza√ß√£o
# Transforma os dados de modo que eles tenham m√©dia zero e desvio padr√£o igual a um.
# - Quando usar    : Aplic√°vel quando os dados j√° est√£o centralizados em torno de uma m√©dia e precisam de ajuste na escala. √â √∫til em modelos como SVM e
#                    Regress√£o Log√≠stica, que s√£o sens√≠veis a varia√ß√µes na escala das vari√°veis de entrada.
# - Exemplo pr√°tico: Se medimos altura em cent√≠metros (150-190 cm) e peso em quilogramas (50-100 kg), a padroniza√ß√£o permite comparar essas medidas numa
#                    escala comum, evitando distor√ß√µes devido a diferentes intervalos de valores.
# - Motivo para
#   este projeto   : Optamos pela padroniza√ß√£o porque as vari√°veis t√™m escalas muito diferentes e h√° a presen√ßa de outliers significativos. A padroniza√ß√£o
#                    mant√©m as propriedades estat√≠sticas dos dados, minimizando o impacto dos outliers, ao contr√°rio da normaliza√ß√£o que pode distorcer os
#                    dados ao comprimir a maioria dos valores em um intervalo estreito.
## Normaliza√ß√£o
# Ajusta os dados para que seus valores caibam em um intervalo predefinido, geralmente de 0 a 1.
# - Quando usar    : Ideal para dados com varia√ß√µes extremas nas escalas e onde os algoritmos s√£o sens√≠veis √† magnitude absoluta dos dados, como
#                    K-Nearest Neighbors (KNN) e t√©cnicas de clustering.
# - Exemplo pr√°tico: Se um dataset cont√©m pre√ßos de produtos variando de R 1ùëéùëÖ1000 e quantidades vendidas de 1 a 20 unidades, a normaliza√ß√£o faria com
#                    que ambos os atributos tivessem a mesma contribui√ß√£o no modelo, independentemente da escala original.
# - Motivo para n√£o
#   esta no projeto: N√£o foi escolhida devido √† presen√ßa de outliers, que poderiam ser enfatizados indevidamente, e porque a normaliza√ß√£o poderia limitar
#                    a efic√°cia de modelos que assumem uma distribui√ß√£o normal dos dados.
## Importante:
# - N√£o √© necess√°rio aplicar padroniza√ß√£o/normaliza√ß√£o na vari√°vel alvo.
# - N√≥s n√£o aplicamos as duas t√©cnicas, ou usamos uma ou outra.
# - A normaliza√ß√£o pode n√£o ser a melhor escolha se houver outliers significativos no conjunto de dados, pois isso poderia comprimir a maioria dos dados
#   em um intervalo muito estreito. Nesses casos, a padroniza√ß√£o √© recomendada.
# Padronizado Dados de Treino
summary(dados_treino)
# Calculando a m√©dia e o desvio padr√£o dos dados de treino
treino_mean <- sapply(dados_treino[, -which(names(dados_treino) == "Target")], mean, na.rm = TRUE)
treino_std <- sapply(dados_treino[, -which(names(dados_treino) == "Target")], sd, na.rm = TRUE)
# Exibindo a m√©dia e o desvio padr√£o
print(treino_mean)
print(treino_std)
# Padronizando todas as vari√°veis, exceto 'Target'
dados_treino[, names(treino_mean)] <- sweep(dados_treino[, names(treino_mean)], 2, treino_mean, "-")
dados_treino[, names(treino_std)] <- sweep(dados_treino[, names(treino_std)], 2, treino_std, "/")
summary(dados_treino)
# Padronizado Dados de Teste
summary(dados_teste)
# Padronizando os dados de teste usando a m√©dia e desvio padr√£o dos dados de treino
dados_teste[, names(treino_mean)] <- sweep(dados_teste[, names(treino_mean)], 2, treino_mean, "-")
dados_teste[, names(treino_std)] <- sweep(dados_teste[, names(treino_std)], 2, treino_std, "/")
summary(dados_teste)
rm(treino_mean, treino_std)
#### Construindo Modelos de Machine Learning
# Nesta etapa do projeto, desenvolveremos e avaliaremos cinco diferentes modelos de machine learning para identificar qual deles apresenta o melhor
# desempenho para o nosso conjunto de dados.
# Cada modelo foi escolhido por suas caracter√≠sticas √∫nicas e capacidade de lidar com problemas de classifica√ß√£o.
# Abaixo est√£o os modelos que ser√£o implementados e testados:
# - Modelo 1: Regress√£o Log√≠stica - Utilizado como benchmark devido √† sua simplicidade e efic√°cia em problemas de classifica√ß√£o bin√°ria.
#   Este modelo ajudar√° a estabelecer uma linha base para a performance que esperamos superar com t√©cnicas mais complexas.
# - Modelo 2: Random Forest - Um modelo de ensemble que usa m√∫ltiplas √°rvores de decis√£o para melhorar a generaliza√ß√£o. √â conhecido por sua alta precis√£o
#   e capacidade de ranquear a import√¢ncia das vari√°veis.
# - Modelo 3: KNN (K-Nearest Neighbors) - Um modelo baseado em inst√¢ncia que faz previs√µes com base nas labels das amostras mais pr√≥ximas no espa√ßo de
#   caracter√≠sticas. Este modelo √© eficaz em casos onde a rela√ß√£o entre as vari√°veis √© altamente n√£o-linear.
# - Modelo 4: Decision Tree (√Årvore de Decis√£o) - Uma √°rvore de decis√£o √© √∫til por sua interpretabilidade, permitindo entender claramente quais crit√©rios
#   o modelo est√° usando para tomar decis√µes.
# - Modelo 5: SVM (Support Vector Machine) - Ideal para problemas de classifica√ß√£o e regress√£o de margem grande. O SVM √© eficiente na cria√ß√£o de hiperplanos
#   em um espa√ßo multidimensional, o que o torna adequado para casos com muitas vari√°veis de entrada.
# Cada modelo ser√° treinado utilizando o mesmo conjunto de dados, permitindo uma compara√ß√£o justa de sua efic√°cia. A avalia√ß√£o de cada modelo incluir√°
# m√©tricas como precis√£o, AUC-ROC, entre outras, dependendo das especificidades de nosso problema e dados.
## Convers√£o da Vari√°vel Alvo Para Tipo Factor (Certifique-se de que Target √© um fator com dois n√≠veis)
dados_treino$Target <- factor(dados_treino$Target, levels = c(0, 1))
dados_teste$Target <- factor(dados_teste$Target, levels = c(0, 1))
# Ajustar os n√≠veis da vari√°vel alvo para serem nomes v√°lidos de vari√°veis em R
levels(dados_treino$Target) <- c("Class0", "Class1")
levels(dados_teste$Target) <- c("Class0", "Class1")
## Cria um dataframe para receber as m√©tricas de cada modelo
df_modelos <- data.frame()
### Modelo 1 com Regress√£o Log√≠stica (Benchmark)
# Para a primeira vers√£o do modelo o ideal √© escolher um algoritmo simples, f√°cil de compreender e que ser√° usado como Benchmark.
# Obs: Como parte do processo envolve aleatoriedade, os resultados podem ser ligeiramente diferentes a cada execu√ß√£o deste bloco de c√≥digo.
## Vers√£o 1
# - Cria v√°rios modelos utilizando o pacote `caret` com um grid de hiperpar√¢metros para `glmnet`, que combina standardiza√ß√£o de dados e regress√£o log√≠stica
#   com penalidade L2. O objetivo √© encontrar os melhores hiperpar√¢metros.
# - O `GridSearchCV` √© simulado no R com a fun√ß√£o `train`, aplicando valida√ß√£o cruzada e pr√©-processamento. Ap√≥s identificar o melhor par√¢metro (lambda),
#   treina-se o modelo final diretamente com a fun√ß√£o `glmnet` utilizando os hiperpar√¢metros otimizados.
# - Isso √© feito para assegurar um modelo eficiente e pronto para implementa√ß√µes pr√°ticas.
str(dados_treino)
str(dados_teste)
# Definindo o controle de treinamento para Grid Search
train_control <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# Define a lista de hiperpar√¢metros
lambda_grid <- 1 / c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000)
tuned_params <- expand.grid(alpha = 0, lambda = lambda_grid)
# Executa o Grid Search usando a f√≥rmula diretamente
grid_search <- train(
Target ~ .,
data = dados_treino,
method = "glmnet",
tuneGrid = tuned_params,
trControl = train_control,
metric = "ROC",
preProcess = "scale",
family = "binomial"
)
# Extraindo os melhores par√¢metros
best_lambda <- grid_search$bestTune$lambda
best_lambda
# Treinando o modelo final com os melhores par√¢metros encontrados
modelo_v1 <- glmnet(
x = as.matrix(dados_treino[, -which(names(dados_treino) == "Target")]),
y = as.numeric(dados_treino$Target),
alpha = 0,  # L2 penalidade como em LogisticRegression com 'l2'
lambda = best_lambda,
family = "binomial",
standardize = TRUE  # Equivalente ao StandardScaler
)
modelo_v1
rm(train_control, lambda_grid, tuned_params, grid_search)
## Previs√µes
# Preparando dados de teste para previs√£o
X_teste <- as.matrix(dados_teste[, -which(names(dados_teste) == "Target")])
# Previs√µes de Classe
y_pred_v1 <- predict(modelo_v1, newx = X_teste, s = "lambda.min", type = "class")
y_pred_v1 <- as.factor(ifelse(y_pred_v1 == "1", "Class0", "Class1"))
head(y_pred_v1)
# Previs√µes de Probabilidade
y_pred_proba_v1 <- predict(modelo_v1, newx = X_teste, s = "lambda.min", type = "response")
head(y_pred_proba_v1)
## Avalia√ß√£o do Modelo
# Matriz de Confus√£o
conf_matrix <- confusionMatrix(y_pred_v1, dados_teste$Target)
conf_matrix
# Calcula e exibe a m√©trica AUC-ROC
roc_obj <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1))
roc_auc_v1 <- auc(roc_obj)
roc_auc_v1
# Calcula a curva ROC
roc_curve <- roc(response = dados_teste$Target, predictor = as.numeric(y_pred_proba_v1))
roc_curve
plot(roc_curve, main="ROC Curve", col="#1c61b6") # Opcional: Gr√°fico da curva ROC
# Calcula e exibe a acur√°cia
acuracia_v1 <- sum(y_pred_v1 == dados_teste$Target) / length(y_pred_v1)
acuracia_v1
# Exibindo os resultados
cat("Confusion Matrix:\n")
print(conf_matrix$table)
cat(sprintf("\nAUC-ROC: %f\n", roc_auc_v1))
cat(sprintf("Accuracy: %f\n", acuracia_v1))
## Salvando as m√©tricas do modelo_v1 em um Dicion√°rio
dict_modelo_v1 <- data.frame(
Nome = "modelo_v1",
Algoritmo = "Regress√£o Log√≠stica",
ROC_AUC_Score = as.numeric(roc_auc_v1),  # Converte AUC para num√©rico
AUC_Score = as.numeric(auc(roc_curve)),  # AUC calculada da curva ROC, tamb√©m convertida
Acuracia = acuracia_v1
)
## Adiciona o Dicion√°rio com resultado das m√©tricas do modelo_v1 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v1)
df_modelos
rm(X_teste, y_pred_v1, y_pred_proba_v1, conf_matrix, roc_obj, acuracia_v1, roc_auc_v1, roc_curve, dict_modelo_v1)
## Vers√£o 2
# - Aplica a T√©cnica de Feature Selection no modelo_v1 criado na Vers√£o 1
# - Re-cria o modelo utilizando as 5 vari√°veis mais importantes
# Extraindo coeficientes do modelo para o melhor lambda
coeficientes <- as.matrix(coef(modelo_v1, s = best_lambda))
rownames(coeficientes) <- c("(Intercept)", names(dados_treino)[-which(names(dados_treino) == "Target")])
# Calculando a import√¢ncia como o valor absoluto dos coeficientes
importancias <- abs(coeficientes[-1, , drop = FALSE])  # Exclui o intercepto
df_importancias <- data.frame(
Feature = rownames(importancias),
Importance = importancias[, 1]
)
df_importancias <- df_importancias[order(-df_importancias$Importance), ]
# Visualizando por N√∫meros
print(df_importancias)
# Visualiando por Gr√°fico
ggplot(df_importancias, aes(x = Importance, y = reorder(Feature, Importance))) +
geom_bar(stat = "identity", fill = "skyblue", orientation = "y") +
labs(title = "Import√¢ncia das Vari√°veis", x = "Import√¢ncia", y = "Vari√°vel") +
theme_minimal() +
theme(axis.text.y = element_text(angle = 0, hjust = 1))  # Melhorando a legibilidade dos r√≥tulos
## Selecionando as 5 vari√°veis mais importantes
# Crit√©rio: Foi detectado uma disparidade entre as 5 primeiras e as outras 4 vari√°veis. Vamos escolher as 5 primerias.
vars_importantes <- head(df_importancias$Feature, 5)
## Recriando o modelo usando apenas as vari√°veis mais importantes
dados_treino_importantes <- dados_treino[, c(vars_importantes, "Target")]
# Recriando modelo_v2 com vari√°veis selecionadas
modelo_v2 <- glmnet(
x = as.matrix(dados_treino_importantes[, -which(names(dados_treino_importantes) == "Target")]),
y = as.numeric(dados_treino_importantes$Target),
alpha = 0,  # L2 penalidade como em LogisticRegression com 'l2'
lambda = best_lambda,
family = "binomial",
standardize = TRUE
)
modelo_v2
## Preparando dados de teste para previs√£o com as vari√°veis mais importantes
dados_teste_importantes <- dados_teste[, c(vars_importantes, "Target")]
X_teste_importantes <- as.matrix(dados_teste_importantes[, -which(names(dados_teste_importantes) == "Target")])
## Previs√µes de Classe
y_pred_v2 <- predict(modelo_v2, newx = X_teste_importantes, s = "lambda.min", type = "class")
y_pred_v2 <- as.factor(ifelse(y_pred_v2 == "1", "Class0", "Class1"))
# Previs√µes de Probabilidade
y_pred_proba_v2 <- predict(modelo_v2, newx = X_teste_importantes, s = "lambda.min", type = "response")
## Avalia√ß√£o do Modelo
conf_matrix_v2 <- confusionMatrix(y_pred_v2, dados_teste_importantes$Target)
roc_obj_v2 <- roc(response = dados_teste_importantes$Target, predictor = as.numeric(y_pred_proba_v2))
roc_auc_v2 <- auc(roc_obj_v2)
acuracia_v2 <- sum(y_pred_v2 == dados_teste_importantes$Target) / length(y_pred_v2)
# Salvando as m√©tricas do modelo_v2 em um Dicion√°rio
dict_modelo_v2 <- data.frame(
Nome = "modelo_v2",
Algoritmo = "Regress√£o Log√≠stica com Sele√ß√£o de Vari√°veis",
ROC_AUC_Score = as.numeric(roc_auc_v2),
AUC_Score = as.numeric(auc(roc_obj_v2)),
Acuracia = acuracia_v2
)
# Adiciona o Dicion√°rio com resultado das m√©tricas do modelo_v2 no dataframe com resultados
df_modelos <- bind_rows(df_modelos, dict_modelo_v2)
df_modelos
rm(modelo_v1, modelo_v2, dados_teste_importantes, X_teste_importantes, importancias, y_pred_v2, y_pred_proba_v2, conf_matrix_v2, roc_obj_v2,
roc_auc_v2, acuracia_v2, dict_modelo_v2, best_lambda, coeficientes, df_importancias, vars_importantes, dados_treino_importantes)
###  Modelo 2 com Random Forest
# - Nosso desafio agora √© tentar obter um modelo melhor que a vers√£o 1. Vamos tentar o algoritmo Random Forest.
## Vers√£o 1
# - Cria√ß√£o e treinamento do modelo com Random Forest
set.seed(123)
# Ajustando Hiperpar√¢metros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o par√¢metro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Verificando a estrutura de tuned_params
str(tuned_params)
# Treinamento com Randomized Search
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Treinamento com Randomized Search
attr(tuned_params, "out.attrs") <- NULL
# Treinamento com Randomized Search
attr(tuned_params, "out.attrs") <- NULL
# Verificando a estrutura de tuned_params
str(tuned_params)
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Ajustando Hiperpar√¢metros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o par√¢metro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Renomeando a coluna mtry para "mtry"
colnames(tuned_params)[colnames(tuned_params) == "mtry"] <- "mtry"
str(tuned_params)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Ajustando Hiperpar√¢metros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o par√¢metro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Renomeando a coluna mtry para "mtry"
attr(tuned_params, "out.attrs") <- NULL
colnames(tuned_params)[colnames(tuned_params) == "mtry"] <- "mtry"
str(tuned_params)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Ajustando Hiperpar√¢metros do Modelo
tuned_params <- expand.grid(
mtry = c(2, 3, 4),  # Adicionando o par√¢metro mtry
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Adicionando uma coluna mtry adicional para satisfazer as expectativas do caret
tuned_params$mtry <- sample(c(2, 3, 4), size = nrow(tuned_params), replace = TRUE)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo_v1 <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params,
metric = "ROC"
)
# Define o grid de hiperpar√¢metros
tuned_params_v2 <- expand.grid(
n_estimators = c(100, 200, 300, 400, 500),
min_samples_split = c(2, 5, 10),
min_samples_leaf = c(1, 2, 4)
)
# Treinamento com Randomized Search
train_control <- trainControl(
method = "cv",
number = 5,
search = "random",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
modelo <- train(
Target ~ .,
data = dados_treino,
method = "rf",
trControl = train_control,
tuneGrid = tuned_params_v2,
metric = "ROC"
)
